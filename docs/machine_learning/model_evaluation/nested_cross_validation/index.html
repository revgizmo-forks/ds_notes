<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Chris Albon  | Nested Cross Validation</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.56.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="Nested Cross Validation" />
<meta property="og:description" content="Nested Cross Validation using scikit-learn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/machine_learning/model_evaluation/nested_cross_validation/" />
<meta property="article:published_time" content="2017-12-20T11:53:49-07:00" />
<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="name" content="Nested Cross Validation">
<meta itemprop="description" content="Nested Cross Validation using scikit-learn.">


<meta itemprop="datePublished" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="dateModified" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="wordCount" content="569">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Nested Cross Validation"/>
<meta name="twitter:description" content="Nested Cross Validation using scikit-learn."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="" class="f3 fw2 hover-white no-underline white-90 dib">
      Chris Albon
    </a>
    <div class="flex-l items-center">
      

      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        MACHINE_LEARNINGS
      </p>
      <h1 class="f1 athelas mb1">Nested Cross Validation</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2017-12-20T11:53:49-07:00">December 20, 2017</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<p>Often we want to tune the parameters of a model (for example, <code>C</code> in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation:</p>

<ol>
<li>Set the parameter you want to tune to some value.</li>
<li>Split your data into K &lsquo;folds&rsquo; (sections).</li>
<li>Train your model using K-1 folds using the parameter value.</li>
<li>Test your model on the remaining fold.</li>
<li>Repeat steps 3 and 4 so that every fold is the test data once.</li>
<li>Repeat steps 1 to 5 for every possible value of the parameter.</li>
<li>Report the parameter that produced the best result.</li>
</ol>

<p>However, as <a href="http://jmlr.org/papers/volume11/cawley10a/cawley10a.pdf">Cawley and Talbot</a> point out in their 2010 paper, since we used the test set to <em>both</em> select the values of the parameter <em>and</em> evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a <em>different</em> test set to get an unbiased evaluation of that selected model.</p>

<p>One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation.</p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Load required packages</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span></code></pre></div>
<h2 id="get-data">Get Data</h2>

<p>The data for this tutorial is beast cancer data with <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">30 features and a binary target variable</a>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Load the data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>

<span class="c1"># Create X from the features</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>

<span class="c1"># Create y from the target</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span></code></pre></div>
<h2 id="standardize-data">Standardize Data</h2>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Create a scaler object</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Fit the scaler to the feature data and transform</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></code></pre></div>
<h2 id="create-inner-cross-validation-for-parameter-tuning">Create Inner Cross Validation (For Parameter Tuning)</h2>

<p>This is our inner cross validation. We will use this to hunt for the best parameters for <code>C</code>, the penalty for misclassifying a data point. <code>GridSearchCV</code> will conduct steps 1-6 listed at the top of this tutorial.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Create a list of 10 candidate values for the C parameter</span>
<span class="n">C_candidates</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Create a gridsearch object with the support vector classifier and the C value candidates</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">SVC</span><span class="p">(),</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">C_candidates</span><span class="p">)</span></code></pre></div>
<p>The code below isn&rsquo;t necessary for parameter tuning using nested cross validation, however to demonstrate that our inner cross validation grid search can find the best value for the parameter <code>C</code>, we will run it once here:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Fit the cross validated grid search on the data </span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_std</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Show the best value for C</span>
<span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">C</span></code></pre></div>
<pre><code>2.7825594022071258
</code></pre>

<h2 id="create-outer-cross-validation-for-model-evaluation">Create Outer Cross Validation (For Model Evaluation)</h2>

<p>With our inner cross validation constructed, we can use <code>cross_val_score</code> to evaluate the model with a second (outer) cross validation.</p>

<p>The code below splits the data into three folds, running the inner cross validation on two of the folds (merged together) and then evaluating the model on the third fold. This is repeated three times so that every fold is used for testing once.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_std</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>array([ 0.94736842,  0.97894737,  0.98412698])
</code></pre>

<p>Each the values above is an unbiased evaluation of the model&rsquo;s accuracy, once for each of the three test folds. Averaged together, they would represent the average accuracy of the model found in the inner cross validated grid search.</p>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="" >
    &copy; 2019 Chris Albon
  </a>
    <div>










</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
