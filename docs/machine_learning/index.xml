<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine_learnings on Chris Albon</title>
    <link>/machine_learning/index.xml</link>
    <description>Recent content in Machine_learnings on Chris Albon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Dec 2017 11:53:49 -0700</lastBuildDate>
    
	<atom:link href="/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ANOVA F-value For Feature Selection</title>
      <link>/machine_learning/feature_selection/anova_f-value_for_feature_selection/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/anova_f-value_for_feature_selection/</guid>
      <description>If the features are categorical, calculate a chi-square ($\chi^{2}$) statistic between each feature and the target vector. However, if the features are quantitative, compute the ANOVA F-value between each feature and the target vector.
The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different.
Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.</description>
    </item>
    
    <item>
      <title>Accuracy</title>
      <link>/machine_learning/model_evaluation/accuracy/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/accuracy/</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Accuracy # Cross-validate model using accuracy cross_val_score(logit, X, y, scoring=&amp;#34;accuracy&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Adaboost Classifier</title>
      <link>/machine_learning/trees_and_forests/adaboost_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/adaboost_classifier/</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import AdaBoostClassifier from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Adaboost Classifier The most important parameters are base_estimator, n_estimators, and learning_rate.
 base_estimator is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree &amp;ndash; this parameter&amp;rsquo;s default argument.</description>
    </item>
    
    <item>
      <title>Adding And Subtracting Matrices</title>
      <link>/machine_learning/vectors_matrices_and_arrays/adding_and_subtracting_matrices/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/adding_and_subtracting_matrices/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrices # Create matrix matrix_a = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 2]]) # Create matrix matrix_b = np.array([[1, 3, 1], [1, 3, 1], [1, 3, 8]]) Add Matrices # Add two matrices np.add(matrix_a, matrix_b) array([[ 2, 4, 2], [ 2, 4, 2], [ 2, 4, 10]])  Subtract Matrices # Subtract two matrices np.subtract(matrix_a, matrix_b) array([[ 0, -2, 0], [ 0, -2, 0], [ 0, -2, -6]])  </description>
    </item>
    
    <item>
      <title>Adding Interaction Terms</title>
      <link>/machine_learning/linear_regression/adding_interaction_terms/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/adding_interaction_terms/</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston from sklearn.preprocessing import PolynomialFeatures import warnings # Suppress Warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Load Boston Housing Dataset # Load the data with only two features boston = load_boston() X = boston.data[:,0:2] y = boston.target Add Interaction Term Interaction effects can be account for by including a new feature comprising the product of corresponding values from the interacting features:</description>
    </item>
    
    <item>
      <title>Agglomerative Clustering</title>
      <link>/machine_learning/clustering/agglomerative_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/agglomerative_clustering/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import AgglomerativeClustering Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct Agglomerative Clustering In scikit-learn, AgglomerativeClustering uses the linkage parameter to determine the merging strategy to minimize the 1) variance of merged clusters (ward), 2) average of distance between observations from pairs of clusters (average), or 3) maximum distance between observations from pairs of clusters (complete).</description>
    </item>
    
    <item>
      <title>Apply Operations To Elements</title>
      <link>/machine_learning/vectors_matrices_and_arrays/apply_operations_to_elements/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/apply_operations_to_elements/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Create Vectorized Function # Create a function that adds 100 to something add_100 = lambda i: i + 100 # Create a vectorized function vectorized_add_100 = np.vectorize(add_100) Apply Function To Elements # Apply function to all elements in matrix vectorized_add_100(matrix) array([[101, 102, 103], [104, 105, 106], [107, 108, 109]])  </description>
    </item>
    
    <item>
      <title>Bag Of Words</title>
      <link>/machine_learning/preprocessing_text/bag_of_words/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/bag_of_words/</guid>
      <description>Preliminaries # Load library import numpy as np from sklearn.feature_extraction.text import CountVectorizer import pandas as pd Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Sweden is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Bag Of Words # Create the bag of words feature matrix count = CountVectorizer() bag_of_words = count.fit_transform(text_data) # Show feature matrix bag_of_words.toarray() array([[0, 0, 0, 2, 0, 0, 1, 0], [0, 1, 0, 0, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)  View Bag Of Words Matrix Column Headers # Get feature names feature_names = count.</description>
    </item>
    
    <item>
      <title>Bernoulli Naive Bayes Classifier</title>
      <link>/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier/</guid>
      <description>The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).
Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import BernoulliNB Create Binary Feature And Target Data # Create three binary features X = np.random.randint(2, size=(100, 3)) # Create a binary target vector y = np.random.randint(2, size=(100, 1)).ravel() View Feature Data # View first ten observations X[0:10] array([[1, 1, 1], [0, 1, 0], [1, 1, 1], [0, 0, 0], [1, 0, 1], [1, 1, 1], [0, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0]])  Train Bernoulli Naive Bayes Classifier # Create Bernoulli Naive Bayes object with prior probabilities of each class clf = BernoulliNB(class_prior=[0.</description>
    </item>
    
    <item>
      <title>Binarize Images</title>
      <link>/machine_learning/preprocessing_images/binarize_image/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/binarize_image/</guid>
      <description> Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as greyscale image_grey = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Apply Adaptive Thresholding # Apply adaptive thresholding max_output_value = 255 neighorhood_size = 99 subtract_from_mean = 10 image_binarized = cv2.adaptiveThreshold(image_grey, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighorhood_size, subtract_from_mean) View Image # Show image plt.imshow(image_binarized, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() </description>
    </item>
    
    <item>
      <title>Blurring Images</title>
      <link>/machine_learning/preprocessing_images/blurring_images/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/blurring_images/</guid>
      <description> Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Blur Image # Blur image image_blurry = cv2.blur(image, (5,5)) View Image # Show image plt.imshow(image_blurry, cmap=&amp;#39;gray&amp;#39;), plt.xticks([]), plt.yticks([]) plt.show() </description>
    </item>
    
    <item>
      <title>Break Up Dates And Times Into Multiple Features</title>
      <link>/machine_learning/preprocessing_dates_and_times/break_up_dates_and_times_into_multiple_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/break_up_dates_and_times_into_multiple_features/</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd.DataFrame() # Create five dates df[&amp;#39;date&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=150, freq=&amp;#39;W&amp;#39;) Break Up Dates And Times Into Individual Features # Create features for year, month, day, hour, and minute df[&amp;#39;year&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.year df[&amp;#39;month&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.month df[&amp;#39;day&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.day df[&amp;#39;hour&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.hour df[&amp;#39;minute&amp;#39;] = df[&amp;#39;date&amp;#39;].dt.minute # Show three rows df.head(3)   .</description>
    </item>
    
    <item>
      <title>Calculate Difference Between Dates And Times</title>
      <link>/machine_learning/preprocessing_dates_and_times/calculate_difference_between_dates_and_times/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/calculate_difference_between_dates_and_times/</guid>
      <description> Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd.DataFrame() # Create two datetime features df[&amp;#39;Arrived&amp;#39;] = [pd.Timestamp(&amp;#39;01-01-2017&amp;#39;), pd.Timestamp(&amp;#39;01-04-2017&amp;#39;)] df[&amp;#39;Left&amp;#39;] = [pd.Timestamp(&amp;#39;01-01-2017&amp;#39;), pd.Timestamp(&amp;#39;01-06-2017&amp;#39;)] Calculate Difference (Method 1) # Calculate duration between features df[&amp;#39;Left&amp;#39;] - df[&amp;#39;Arrived&amp;#39;] 0 0 days 1 2 days dtype: timedelta64[ns]  Calculate Difference (Method 2) # Calculate duration between features pd.Series(delta.days for delta in (df[&amp;#39;Left&amp;#39;] - df[&amp;#39;Arrived&amp;#39;])) 0 0 1 2 dtype: int64  </description>
    </item>
    
    <item>
      <title>Calculate Dot Product Of Two Vectors</title>
      <link>/machine_learning/vectors_matrices_and_arrays/calculate_dot_product_of_two_vectors/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/calculate_dot_product_of_two_vectors/</guid>
      <description> Preliminaries # Load library import numpy as np Create Two Vectors # Create two vectors vector_a = np.array([1,2,3]) vector_b = np.array([4,5,6]) Calculate Dot Product (Method 1) # Calculate dot product np.dot(vector_a, vector_b) 32  Calculate Dot Product (Method 2) # Calculate dot product vector_a @ vector_b 32  </description>
    </item>
    
    <item>
      <title>Calculate The Average, Variance, And Standard Deviation</title>
      <link>/machine_learning/vectors_matrices_and_arrays/calculate_average_variance_and_standard_deviation/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/calculate_average_variance_and_standard_deviation/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Calculate Mean # Return mean np.mean(matrix) 5.0  Calculate Variance # Return variance np.var(matrix) 6.666666666666667  Calculate Standard Deviation # Return standard deviation np.std(matrix) 2.5819888974716112  </description>
    </item>
    
    <item>
      <title>Calculate The Determinant Of A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/calculate_the_determinant_of_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/calculate_the_determinant_of_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Calculate Determinant # Return determinant of matrix np.linalg.det(matrix) -9.5161973539299405e-16  </description>
    </item>
    
    <item>
      <title>Calculate The Trace Of A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/calculate_the_trace_of_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/calculate_the_trace_of_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Calculate The Trace # Calculate the trace of the matrix matrix.diagonal().sum() 15  </description>
    </item>
    
    <item>
      <title>Calibrate Predicted Probabilities</title>
      <link>/machine_learning/naive_bayes/calibrate_predicted_probabilities/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/calibrate_predicted_probabilities/</guid>
      <description>Class probabilities are a common and useful part of machine learning models. In scikit-learn, most learning algortihms allow us to see the predicted probabilities of class membership using predict_proba. This can be extremely useful if, for instance, we want to only predict a certain class if the model predicts the probability that they are that class is over 90%. However, some models, including naive Bayes classifiers output probabilities that are not based on the real world.</description>
    </item>
    
    <item>
      <title>Calibrate Predicted Probabilities In SVC</title>
      <link>/machine_learning/support_vector_machines/calibrate_predicted_probabilities_in_svc/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/calibrate_predicted_probabilities_in_svc/</guid>
      <description>SVC&amp;rsquo;s use of a hyperplane to create decision regions do not naturally output a probability estimate that an observation is a member of a certain class. However, we can in fact output calibrated class probabilities with a few caveats. In an SVC, Platt scaling can be used, wherein first the SVC is trained, then a separate cross-validated logistic regression is trained to map the SVC outputs into probabilities:
$$P(y=1 \mid x)={\frac {1}{1+e^{(A*f(x)+B)}}}$$</description>
    </item>
    
    <item>
      <title>Chi-Squared For Feature Selection</title>
      <link>/machine_learning/feature_selection/chi-squared_for_feature_selection/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/chi-squared_for_feature_selection/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 Load Data # Load iris data iris = load_iris() # Create features and target X = iris.data y = iris.target # Convert to categorical data by converting data to integers X = X.astype(int) Compare Chi-Squared Statistics # Select two features with highest chi-squared statistics chi2_selector = SelectKBest(chi2, k=2) X_kbest = chi2_selector.fit_transform(X, y) View Results # Show results print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Convert Pandas Categorical Data For Scikit-Learn</title>
      <link>/machine_learning/preprocessing_structured_data/convert_pandas_categorical_column_into_integers_for_scikit-learn/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/convert_pandas_categorical_column_into_integers_for_scikit-learn/</guid>
      <description>Preliminaries # Import required packages from sklearn import preprocessing import pandas as pd Create DataFrame raw_data = {&amp;#39;patient&amp;#39;: [1, 1, 1, 2, 2], &amp;#39;obs&amp;#39;: [1, 2, 3, 1, 2], &amp;#39;treatment&amp;#39;: [0, 1, 0, 1, 0], &amp;#39;score&amp;#39;: [&amp;#39;strong&amp;#39;, &amp;#39;weak&amp;#39;, &amp;#39;normal&amp;#39;, &amp;#39;weak&amp;#39;, &amp;#39;strong&amp;#39;]} df = pd.DataFrame(raw_data, columns = [&amp;#39;patient&amp;#39;, &amp;#39;obs&amp;#39;, &amp;#39;treatment&amp;#39;, &amp;#39;score&amp;#39;]) Fit The Label Encoder # Create a label (category) encoder object le = preprocessing.LabelEncoder()# Fit the encoder to the pandas column le.</description>
    </item>
    
    <item>
      <title>Convert Strings To Dates</title>
      <link>/machine_learning/preprocessing_dates_and_times/convert_strings_to_dates/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/convert_strings_to_dates/</guid>
      <description>Preliminaries # Load libraries import numpy as np import pandas as pd Create Strings # Create strings date_strings = np.array([&amp;#39;03-04-2005 11:35 PM&amp;#39;, &amp;#39;23-05-2010 12:01 AM&amp;#39;, &amp;#39;04-09-2009 09:09 PM&amp;#39;]) Convert Strings To Timestamps If errors=&amp;quot;coerce&amp;quot; then any problem will not raise an error (the default behavior) but instead will set the value causing the error to NaT (i.e. a missing value).
  Code Description Example   %Y Full year `2001`   %m Month w/ zero padding `04`   %d Day of the month w/ zero padding `09`   %I Hour (12hr clock) w/ zero padding `02`   %p AM or PM `AM`   %M Minute w/ zero padding `05`   %S Second w/ zero padding `09`   # Convert to datetimes [pd.</description>
    </item>
    
    <item>
      <title>Convert pandas Columns Time Zone</title>
      <link>/machine_learning/preprocessing_dates_and_times/convert_pandas_column_timezone/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/convert_pandas_column_timezone/</guid>
      <description>Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones[0:10] [&#39;Africa/Abidjan&#39;, &#39;Africa/Accra&#39;, &#39;Africa/Addis_Ababa&#39;, &#39;Africa/Algiers&#39;, &#39;Africa/Asmara&#39;, &#39;Africa/Asmera&#39;, &#39;Africa/Bamako&#39;, &#39;Africa/Bangui&#39;, &#39;Africa/Banjul&#39;, &#39;Africa/Bissau&#39;]  Create pandas Series Of Dates # Create ten dates dates = pd.Series(pd.date_range(&amp;#39;2/2/2002&amp;#39;, periods=10, freq=&amp;#39;M&amp;#39;)) Add Time Zone Of pandas Series # Set time zone dates_with_abidjan_time_zone = dates.dt.tz_localize(&amp;#39;Africa/Abidjan&amp;#39;) # View pandas series dates_with_abidjan_time_zone 0 2002-02-28 00:00:00+00:00 1 2002-03-31 00:00:00+00:00 2 2002-04-30 00:00:00+00:00 3 2002-05-31 00:00:00+00:00 4 2002-06-30 00:00:00+00:00 5 2002-07-31 00:00:00+00:00 6 2002-08-31 00:00:00+00:00 7 2002-09-30 00:00:00+00:00 8 2002-10-31 00:00:00+00:00 9 2002-11-30 00:00:00+00:00 dtype: datetime64[ns, Africa/Abidjan]  Convert Time Zone Of pandas Series # Convert time zone dates_with_london_time_zone = dates_with_abidjan_time_zone.</description>
    </item>
    
    <item>
      <title>Converting A Dictionary Into A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/converting_a_dictionary_into_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/converting_a_dictionary_into_a_matrix/</guid>
      <description>Preliminaries # Load library from sklearn.feature_extraction import DictVectorizer Create Dictionary # Our dictionary of data data_dict = [{&amp;#39;Red&amp;#39;: 2, &amp;#39;Blue&amp;#39;: 4}, {&amp;#39;Red&amp;#39;: 4, &amp;#39;Blue&amp;#39;: 3}, {&amp;#39;Red&amp;#39;: 1, &amp;#39;Yellow&amp;#39;: 2}, {&amp;#39;Red&amp;#39;: 2, &amp;#39;Yellow&amp;#39;: 2}] Feature Matrix From Dictionary # Create DictVectorizer object dictvectorizer = DictVectorizer(sparse=False) # Convert dictionary into feature matrix features = dictvectorizer.fit_transform(data_dict) # View feature matrix features array([[ 4., 2., 0.], [ 3., 4., 0.], [ 0., 1., 2.</description>
    </item>
    
    <item>
      <title>Create A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/create_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/create_a_matrix/</guid>
      <description>Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 4], [2, 5]]) Note NumPy&amp;rsquo;s mat data structure is less flexible for our purposes and should be avoided.</description>
    </item>
    
    <item>
      <title>Create A Sparse Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/create_a_sparse_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/create_a_sparse_matrix/</guid>
      <description>Preliminaries # Load libraries import numpy as np from scipy import sparse Create Dense Matrix # Create a matrix matrix = np.array([[0, 0], [0, 1], [3, 0]]) Convert To Sparse Matrix # Create compressed sparse row (CSR) matrix matrix_sparse = sparse.csr_matrix(matrix) Note: There are many types of sparse matrices. In the example above we use CSR but the type we use should reflect our use case.</description>
    </item>
    
    <item>
      <title>Create A Vector</title>
      <link>/machine_learning/vectors_matrices_and_arrays/create_a_vector/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/create_a_vector/</guid>
      <description> Preliminaries # Load library import numpy as np Create Row Vector # Create a vector as a row vector_row = np.array([1, 2, 3]) Create Column Vector # Create a vector as a column vector_column = np.array([[1], [2], [3]])</description>
    </item>
    
    <item>
      <title>Create Baseline Classification Model</title>
      <link>/machine_learning/model_evaluation/create_baseline_classification_model/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/create_baseline_classification_model/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.dummy import DummyClassifier from sklearn.model_selection import train_test_split Load Iris Flower Dataset # Load data iris = load_iris() # Create target vector and feature matrix X, y = iris.data, iris.target Split Data Into Training And Test Set # Split into training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Create Dummy Regression Always Predicts The Mean Value Of Target # Create dummy classifer dummy = DummyClassifier(strategy=&amp;#39;uniform&amp;#39;, random_state=1) # &amp;#34;Train&amp;#34; model dummy.</description>
    </item>
    
    <item>
      <title>Create Baseline Regression Model</title>
      <link>/machine_learning/model_evaluation/create_baseline_regression_model/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/create_baseline_regression_model/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import load_boston from sklearn.dummy import DummyRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() # Create features X, y = boston.data, boston.target Split Data Into Training And Test Set # Make test and training split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Create Dummy Regression Always Predicts The Mean Value Of Target # Create a dummy regressor dummy_mean = DummyRegressor(strategy=&amp;#39;mean&amp;#39;) # &amp;#34;Train&amp;#34; dummy regressor dummy_mean.</description>
    </item>
    
    <item>
      <title>Create Interaction Features</title>
      <link>/machine_learning/linear_regression/create_interaction_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/create_interaction_features/</guid>
      <description> Preliminaries # Load libraries from sklearn.preprocessing import PolynomialFeatures import numpy as np Create Feature Matrix # Create feature matrix X = np.array([[2, 3], [2, 3], [2, 3]]) Add Interaction Features # Create PolynomialFeatures object with interaction_only set to True interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) # Transform feature matrix interaction.fit_transform(X) array([[ 2., 3., 6.], [ 2., 3., 6.], [ 2., 3., 6.]])  </description>
    </item>
    
    <item>
      <title>Cropping Images</title>
      <link>/machine_learning/preprocessing_images/cropping_images/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/cropping_images/</guid>
      <description> Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Crop Image # Select first half of the columns and all rows image_cropped = image[:,:126] View Image # View image plt.imshow(image_cropped, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() </description>
    </item>
    
    <item>
      <title>Cross Validation Pipeline</title>
      <link>/machine_learning/model_evaluation/cross_validation_pipeline/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/cross_validation_pipeline/</guid>
      <description>The code below does a lot in only a few lines. To help explain things, here are the steps that code is doing:
 Split the raw data into three folds. Select one for testing and two for training. Preprocess the data by scaling the training features. Train a support vector classifier on the training data. Apply the classifier to the test data. Record the accuracy score. Repeat steps 1-5 two more times, once for each fold.</description>
    </item>
    
    <item>
      <title>Cross Validation With Parameter Tuning Using Grid Search</title>
      <link>/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/cross_validation_parameter_tuning_grid_search/</guid>
      <description>In machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model&amp;rsquo;s parameters that maximize the accuracy of the model.
In this tutorial we work through an example which combines cross validation and parameter tuning using scikit-learn.</description>
    </item>
    
    <item>
      <title>Cross-Validation</title>
      <link>/machine_learning/model_evaluation/cross-validaton/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/cross-validaton/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn import metrics from sklearn.model_selection import KFold, cross_val_score from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler Load Digits Dataset # Load the digits dataset digits = datasets.load_digits() # Create the features matrix X = digits.data # Create the target vector y = digits.target Create Pipeline # Create standardizer standardizer = StandardScaler() # Create logistic regression logit = LogisticRegression() # Create a pipeline that standardizes, then runs logistic regression pipeline = make_pipeline(standardizer, logit) Create k-Fold Cross-Validation # Create k-Fold cross-validation kf = KFold(n_splits=10, shuffle=True, random_state=1) Conduct k-Fold Cross-Validation # Do k-fold cross-validation cv_results = cross_val_score(pipeline, # Pipeline X, # Feature matrix y, # Target vector cv=kf, # Cross-validation technique scoring=&amp;#34;accuracy&amp;#34;, # Loss function n_jobs=-1) # Use all CPU scores Calculate Mean Performance Score # Calculate mean cv_results.</description>
    </item>
    
    <item>
      <title>Custom Performance Metric</title>
      <link>/machine_learning/model_evaluation/custom_performance_metric/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/custom_performance_metric/</guid>
      <description>Preliminaries # Load libraries from sklearn.metrics import make_scorer, r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.datasets import make_regression Create Feature # Generate features matrix and target vector X, y = make_regression(n_samples = 100, n_features = 3, random_state = 1) # Create training set and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1) Train model # Create ridge regression object classifier = Ridge() # Train ridge regression model model = classifier.</description>
    </item>
    
    <item>
      <title>DBSCAN Clustering</title>
      <link>/machine_learning/clustering/dbscan_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/dbscan_clustering/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import DBSCAN Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct Meanshift Clustering DBSCAN has three main parameters to set:
 eps: The maximum distance from an observation for another observation to be considered its neighbor. min_samples: The minimum number of observation less than eps distance from an observation for to be considered a core observation.</description>
    </item>
    
    <item>
      <title>Decision Tree Classifier</title>
      <link>/machine_learning/trees_and_forests/decision_tree_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/decision_tree_classifier/</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Decision Tree Using Gini Impurity # Create decision tree classifer object using gini clf = DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;, random_state=0) Train Model # Train model model = clf.fit(X, y) Create Observation To Predict # Make new observation observation = [[ 5, 4, 3, 2]] Predict Observation # Predict observation&amp;#39;s class  model.</description>
    </item>
    
    <item>
      <title>Decision Tree Regression</title>
      <link>/machine_learning/trees_and_forests/decision_tree_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/decision_tree_regression/</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeRegressor from sklearn import datasets Load Boston Housing Dataset # Load data with only two features boston = datasets.load_boston() X = boston.data[:,0:2] y = boston.target Create Decision Tree Decision tree regression works similar to decision tree classification, however instead of reducing Gini impurity or entropy, potential splits are measured on how much they reduce the mean squared error (MSE):
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$</description>
    </item>
    
    <item>
      <title>Delete Observations With Missing Values</title>
      <link>/machine_learning/preprocessing_structured_data/delete_observations_with_missing_values/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/delete_observations_with_missing_values/</guid>
      <description> Preliminaries # Load libraries import numpy as np import pandas as pd Create Feature Matrix # Create feature matrix X = np.array([[1.1, 11.1], [2.2, 22.2], [3.3, 33.3], [4.4, 44.4], [np.nan, 55]]) Delete Observations With Missing Values # Remove observations with missing values X[~np.isnan(X).any(axis=1)] array([[ 1.1, 11.1], [ 2.2, 22.2], [ 3.3, 33.3], [ 4.4, 44.4]])  </description>
    </item>
    
    <item>
      <title>Deleting Missing Values</title>
      <link>/machine_learning/preprocessing_structured_data/deleting_missing_values/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/deleting_missing_values/</guid>
      <description>Preliminaries # Load library import numpy as np import pandas as pd Create Data Frame # Create feature matrix X = np.array([[1, 2], [6, 3], [8, 4], [9, 5], [np.nan, 4]]) Drop Missing Values Using NumPy # Remove observations with missing values X[~np.isnan(X).any(axis=1)] array([[ 1., 2.], [ 6., 3.], [ 8., 4.], [ 9., 5.]])  Drop Missing Values Using pandas # Load data as a data frame df = pd.</description>
    </item>
    
    <item>
      <title>Describe An Array</title>
      <link>/machine_learning/vectors_matrices_and_arrays/describe_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/describe_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) View Shape # View number of rows and columns matrix.shape (3, 4)  View Total Elements # View number of elements (rows * columns) matrix.size 12  View Number Of Dimensions # View number of dimensions matrix.ndim 2  </description>
    </item>
    
    <item>
      <title>Detect Edges</title>
      <link>/machine_learning/preprocessing_images/detect_edges/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/detect_edges/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as greyscale image_gray = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Detect Edges # Calculate median intensity median_intensity = np.median(image_gray) # Set thresholds to be one standard deviation above and below median intensity lower_threshold = int(max(0, (1.0 - 0.33) * median_intensity)) upper_threshold = int(min(255, (1.0 + 0.33) * median_intensity)) # Apply canny edge detector image_canny = cv2.</description>
    </item>
    
    <item>
      <title>Detecting Outliers</title>
      <link>/machine_learning/preprocessing_structured_data/detecting_outliers/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/detecting_outliers/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.datasets import make_blobs Create Data # Create simulated data X, _ = make_blobs(n_samples = 10, n_features = 2, centers = 1, random_state = 1) # Replace the first observation&amp;#39;s values with extreme values X[0,0] = 10000 X[0,1] = 10000 Detect Outliers EllipticEnvelope assumes the data is normally distributed and based on that assumption &amp;ldquo;draws&amp;rdquo; an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1).</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction On Sparse Feature Matrix</title>
      <link>/machine_learning/feature_engineering/dimensionality_reduction_on_sparse_feature_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/dimensionality_reduction_on_sparse_feature_matrix/</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets.load_digits() # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) # Make sparse matrix X_sparse = csr_matrix(X) Create Truncated Singular Value Decomposition # Create a TSVD tsvd = TruncatedSVD(n_components=10) Run Truncated Singular Value Decomposition # Conduct TSVD on sparse matrix X_sparse_tsvd = tsvd.</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction With Kernel PCA</title>
      <link>/machine_learning/feature_engineering/dimensionality_reduction_with_kernel_pca/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/dimensionality_reduction_with_kernel_pca/</guid>
      <description> 
Preliminaries # Load libraries from sklearn.decomposition import PCA, KernelPCA from sklearn.datasets import make_circles Create Linearly Inseparable Data # Create linearly inseparable data X, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1) Conduct Kernel PCA # Apply kernal PCA with radius basis function (RBF) kernel kpca = KernelPCA(kernel=&amp;#34;rbf&amp;#34;, gamma=15, n_components=1) X_kpca = kpca.fit_transform(X) View Results print(&amp;#39;Original number of features:&amp;#39;, X.shape[1]) print(&amp;#39;Reduced number of features:&amp;#39;, X_kpca.shape[1]) Original number of features: 2 Reduced number of features: 1  </description>
    </item>
    
    <item>
      <title>Dimensionality Reduction With PCA</title>
      <link>/machine_learning/feature_engineering/dimensionality_reduction_with_pca/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/dimensionality_reduction_with_pca/</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn import datasets Load Data # Load the data digits = datasets.load_digits() Standardize Feature Values # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) Conduct Principal Component Analysis # Create a PCA that will retain 99% of the variance pca = PCA(n_components=0.99, whiten=True) # Conduct PCA X_pca = pca.fit_transform(X) View Results # Show results print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Discretize Features</title>
      <link>/machine_learning/preprocessing_structured_data/discretize_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/discretize_features/</guid>
      <description> Preliminaries # Load libraries from sklearn.preprocessing import Binarizer import numpy as np Create Data # Create feature age = np.array([[6], [12], [20], [36], [65]]) Option 1: Binarize Feature # Create binarizer binarizer = Binarizer(18) # Transform feature binarizer.fit_transform(age) array([[0], [0], [1], [1], [1]])  Option 2: Break Up Feature Into Bins # Bin feature np.digitize(age, bins=[20,30,64]) array([[0], [0], [1], [2], [3]])  </description>
    </item>
    
    <item>
      <title>Drop Highly Correlated Features</title>
      <link>/machine_learning/feature_selection/drop_highly_correlated_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/drop_highly_correlated_features/</guid>
      <description>Preliminaries # Load libraries import pandas as pd import numpy as np Load Data # Create feature matrix with two highly correlated features X = np.array([[1, 1, 1], [2, 2, 0], [3, 3, 1], [4, 4, 0], [5, 5, 1], [6, 6, 0], [7, 7, 1], [8, 7, 0], [9, 7, 1]]) # Convert feature matrix into DataFrame df = pd.DataFrame(X) # View the data frame df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .</description>
    </item>
    
    <item>
      <title>Effect Of Alpha On Lasso Regression</title>
      <link>/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/</guid>
      <description>Often we want conduct a process called regularization, wherein we penalize the number of features in a model in order to only keep the most important features. This can be particularly important when you have a dataset with 100,000+ features.
Lasso regression is a common modeling technique to do regularization. The math behind it is pretty interesting, but practically, what you need to know is that Lasso regression comes with a parameter, alpha, and the higher the alpha, the most feature coefficients are zero.</description>
    </item>
    
    <item>
      <title>Encode Days Of The Week</title>
      <link>/machine_learning/preprocessing_dates_and_times/encode_days_of_the_week/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/encode_days_of_the_week/</guid>
      <description> Preliminaries # Load library import pandas as pd Create Date And Time Data # Create dates dates = pd.Series(pd.date_range(&amp;#39;2/2/2002&amp;#39;, periods=3, freq=&amp;#39;M&amp;#39;)) # View data dates 0 2002-02-28 1 2002-03-31 2 2002-04-30 dtype: datetime64[ns]  Show Days Of The Week # Show days of the week dates.dt.weekday_name 0 Thursday 1 Sunday 2 Tuesday dtype: object  </description>
    </item>
    
    <item>
      <title>Encoding Ordinal Categorical Features</title>
      <link>/machine_learning/preprocessing_structured_data/encoding_ordinal_categorical_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/encoding_ordinal_categorical_features/</guid>
      <description>Preliminaries # Load library import pandas as pd Create Feature Matrix # Create features df = pd.DataFrame({&amp;#39;Score&amp;#39;: [&amp;#39;Low&amp;#39;, &amp;#39;Low&amp;#39;, &amp;#39;Medium&amp;#39;, &amp;#39;Medium&amp;#39;, &amp;#39;High&amp;#39;]}) # View data frame df   .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Score     0 Low   1 Low   2 Medium   3 Medium   4 High     Create Scale Map # Create mapper scale_mapper = {&amp;#39;Low&amp;#39;:1, &amp;#39;Medium&amp;#39;:2, &amp;#39;High&amp;#39;:3} Map Scale To Features # Map feature values to scale df[&amp;#39;Scale&amp;#39;] = df[&amp;#39;Score&amp;#39;].</description>
    </item>
    
    <item>
      <title>Enhance Contrast Of Color Image</title>
      <link>/machine_learning/preprocessing_images/enhance_contrast_of_color_image/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/enhance_contrast_of_color_image/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2.imread(&amp;#39;images/plane.jpg&amp;#39;) Convert Image To YUV Color Format # Convert to YUV image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV) Enhance Image # Apply histogram equalization image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0]) Convert To RGB # Convert to RGB image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB) View Image # Show image plt.imshow(image_rgb), plt.axis(&amp;#34;off&amp;#34;) plt.</description>
    </item>
    
    <item>
      <title>Enhance Contrast Of Greyscale Image</title>
      <link>/machine_learning/preprocessing_images/enhance_contrast_of_greyscale_image/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/enhance_contrast_of_greyscale_image/</guid>
      <description> Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Enhance Image # Enhance image image_enhanced = cv2.equalizeHist(image) View Image # Show image plt.imshow(image_enhanced, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() </description>
    </item>
    
    <item>
      <title>Evaluating Clustering</title>
      <link>/machine_learning/clustering/evaluating_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/evaluating_clustering/</guid>
      <description>Preliminaries import numpy as np from sklearn.metrics import silhouette_score from sklearn import datasets from sklearn.cluster import KMeans from sklearn.datasets import make_blobs Create Feature Data # Generate feature matrix X, _ = make_blobs(n_samples = 1000, n_features = 10, centers = 2, cluster_std = 0.5, shuffle = True, random_state = 1) Cluster Observations # Cluster data using k-means to predict classes model = KMeans(n_clusters=2, random_state=1).fit(X) # Get predicted classes y_hat = model.</description>
    </item>
    
    <item>
      <title>F1 Score</title>
      <link>/machine_learning/model_evaluation/f1_score/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/f1_score/</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using F1 # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;f1&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Fast C Hyperparameter Tuning</title>
      <link>/machine_learning/logistic_regression/fast_c_hyperparameter_tuning/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/fast_c_hyperparameter_tuning/</guid>
      <description>Sometimes the characteristics of a learning algorithm allows us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods.
scikit-learn&amp;rsquo;s LogisticRegressionCV method includes a parameter Cs. If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied a integer, Cs a list of that many candidate values will is drawn from a logarithmic scale between 0.0001 and and 10000 (a range of reasonable values for C).</description>
    </item>
    
    <item>
      <title>Feature Extraction With PCA</title>
      <link>/machine_learning/feature_engineering/feature_extraction_with_pca/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/feature_extraction_with_pca/</guid>
      <description>Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions. Practically, PCA converts a matrix of n features into a new dataset of (hopefully) less than n features. That is, it reduces the number of features by constructing a new, smaller number variables which capture a signficant portion of the information found in the original features.</description>
    </item>
    
    <item>
      <title>Feature Importance</title>
      <link>/machine_learning/trees_and_forests/feature_importance/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/feature_importance/</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets import numpy as np import matplotlib.pyplot as plt Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train A Decision Tree Model # Create decision tree classifer object clf = RandomForestClassifier(random_state=0, n_jobs=-1) # Train model model = clf.fit(X, y) View Feature Importance # Calculate feature importances importances = model.feature_importances_ Visualize Feature Importance # Sort feature importances in descending order indices = np.</description>
    </item>
    
    <item>
      <title>Feature Selection Using Random Forest</title>
      <link>/machine_learning/trees_and_forests/feature_selection_using_random_forest/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/feature_selection_using_random_forest/</guid>
      <description>Often in data science we have hundreds or even millions of features and we want a way to create a model that only includes the most important features. This has three benefits. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called &amp;ldquo;feature selection.</description>
    </item>
    
    <item>
      <title>Find Best Preprocessing Steps During Model Selection</title>
      <link>/machine_learning/model_selection/find_best_preprocessing_steps_during_model_selection/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_selection/find_best_preprocessing_steps_during_model_selection/</guid>
      <description>We have to be careful to properly handle preprocessing when conducting model selection. First, GridSearchCV uses cross-validation to determine which model has the highest performance. However, in cross-validation we are in effect pretending that the fold held out as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g. scaling or standardization). For this reason, we cannot preprocess the data then run GridSearchCV.</description>
    </item>
    
    <item>
      <title>Find Nearest Neighbors</title>
      <link>/machine_learning/support_vector_machines/find_nearest_neighbors/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/find_nearest_neighbors/</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import NearestNeighbors from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Iris Data It is important to standardize our data before we calculate any distances.
# Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Find Each Observation&amp;rsquo;s Two Nearest Neighbors # Find three nearest neighbors based on euclidean distance (including itself) nn_euclidean = NearestNeighbors(n_neighbors=3, metric=&amp;#39;euclidean&amp;#39;).</description>
    </item>
    
    <item>
      <title>Find Support Vectors</title>
      <link>/machine_learning/support_vector_machines/find_support_vectors/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/find_support_vectors/</guid>
      <description>Preliminaries # Load libraries from sklearn.svm import SVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Dataset #Load data with only two classes iris = datasets.load_iris() X = iris.data[:100,:] y = iris.target[:100] Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Train Support Vector Classifier # Create support vector classifier object svc = SVC(kernel=&amp;#39;linear&amp;#39;, random_state=0) # Train classifier model = svc.</description>
    </item>
    
    <item>
      <title>Find The Maximum And Minimum</title>
      <link>/machine_learning/vectors_matrices_and_arrays/find_maximum_and_minimum/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/find_maximum_and_minimum/</guid>
      <description>Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Find Maximum Element # Return maximum element np.max(matrix) 9  Find Minimum Element # Return minimum element np.min(matrix) 1  Find Maximum Element By Column # Find the maximum element in each column np.max(matrix, axis=0) array([7, 8, 9])  Find Maximum Element By Row # Find the maximum element in each row np.</description>
    </item>
    
    <item>
      <title>Find The Rank Of A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/find_the_rank_of_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/find_the_rank_of_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Find Rank Of Matrix # Return matrix rank np.linalg.matrix_rank(matrix) 2  </description>
    </item>
    
    <item>
      <title>Flatten A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/flatten_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/flatten_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Flatten Matrix # Flatten matrix matrix.flatten() array([1, 2, 3, 4, 5, 6, 7, 8, 9])  </description>
    </item>
    
    <item>
      <title>Gaussian Naive Bayes Classifier</title>
      <link>/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/</guid>
      <description>Because of the assumption of the normal distribution, Gaussian Naive Bayes is best used in cases when all our features are continuous.
Preliminaries # Load libraries from sklearn import datasets from sklearn.naive_bayes import GaussianNB Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train Gaussian Naive Bayes Classifier # Create Gaussian Naive Bayes object with prior probabilities of each class clf = GaussianNB(priors=[0.</description>
    </item>
    
    <item>
      <title>Generate Text Reports On Performance</title>
      <link>/machine_learning/model_evaluation/generate_text_reports_on_performance/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/generate_text_reports_on_performance/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report Load Iris Flower Data # Load data iris = datasets.load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target # Create list of target class names class_names = iris.target_names Create Training And Test Sets # Create training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) Train A Logistic Regression Model # Create logistic regression classifier = LogisticRegression() # Train model and make predictions y_hat = classifier.</description>
    </item>
    
    <item>
      <title>Getting The Diagonal Of A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/getting_the_diagonal_of_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/getting_the_diagonal_of_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Get The Diagonal # Return diagonal elements matrix.diagonal() array([1, 5, 9])  Calculate The Trace # Calculate the tracre of the matrix matrix.diagonal().sum() 15  </description>
    </item>
    
    <item>
      <title>Group Observations Using K-Means Clustering</title>
      <link>/machine_learning/feature_engineering/group_observations_using_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/group_observations_using_clustering/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_blobs from sklearn.cluster import KMeans import pandas as pd Create Data # Make simulated feature matrix X, _ = make_blobs(n_samples = 50, n_features = 2, centers = 3, random_state = 1) # Create DataFrame df = pd.DataFrame(X, columns=[&amp;#39;feature_1&amp;#39;,&amp;#39;feature_2&amp;#39;]) Train Clusterer # Make k-means clusterer clusterer = KMeans(3, random_state=1) # Fit clusterer clusterer.fit(X) KMeans(algorithm=&#39;auto&#39;, copy_x=True, init=&#39;k-means++&#39;, max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances=&#39;auto&#39;, random_state=1, tol=0.0001, verbose=0)  Create Feature Based On Predicted Cluster # Predict values df[&amp;#39;group&amp;#39;] = clusterer.</description>
    </item>
    
    <item>
      <title>Handle Imbalanced Classes In Random Forest</title>
      <link>/machine_learning/trees_and_forests/handle_imbalanced_classes_in_random_forests/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/handle_imbalanced_classes_in_random_forests/</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Adjust Iris Dataset To Make Classes Imbalanced # Make class highly imbalanced by removing first 40 observations X = X[40:,:] y = y[40:] # Create target vector indicating if class 0, otherwise 1 y = np.where((y == 0), 0, 1) Train Random Forest While Balancing Classes When using RandomForestClassifier a useful setting is class_weight=balanced wherein classes are automatically weighted inversely proportional to how frequently they appear in the data.</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes In Logistic Regression</title>
      <link>/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/handling_imbalanced_classes_in_logistic_regression/</guid>
      <description>Like many other learning algorithms in scikit-learn, LogisticRegression comes with a built-in method of handling imbalanced classes. If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Specifically, the balanced argument will automatically weigh classes inversely proportional to their frequency:
$$w_j = \frac{n}{kn_{j}}$$</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes With Downsampling</title>
      <link>/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling/</guid>
      <description>In downsampling, we randomly sample without replacement from the majority class (i.e. the class with more observations) to create a new subset of observation equal in size to the minority class.
Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target Make Iris Dataset Imbalanced # Remove first 40 observations X = X[40:,:] y = y[40:] # Create binary target vector indicating if class 0 y = np.</description>
    </item>
    
    <item>
      <title>Handling Imbalanced Classes With Upsampling</title>
      <link>/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_upsampling/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_upsampling/</guid>
      <description>In upsampling, for every observation in the majority class, we randomly select an observation from the minority class with replacement. The end result is the same number of observations from the minority and majority classes.
Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.</description>
    </item>
    
    <item>
      <title>Handling Missing Values In Time Series</title>
      <link>/machine_learning/preprocessing_dates_and_times/handling_missing_values_in_time_series/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/handling_missing_values_in_time_series/</guid>
      <description>Preliminaries # Load libraries import pandas as pd import numpy as np Create Date Data With Gap In Values # Create date time_index = pd.date_range(&amp;#39;01/01/2010&amp;#39;, periods=5, freq=&amp;#39;M&amp;#39;) # Create data frame, set index df = pd.DataFrame(index=time_index) # Create feature with a gap of missing values df[&amp;#39;Sales&amp;#39;] = [1.0,2.0,np.nan,np.nan,5.0] Interpolate Missing Values # Interpolate missing values df.interpolate()   .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .</description>
    </item>
    
    <item>
      <title>Handling Outliers</title>
      <link>/machine_learning/preprocessing_structured_data/handling_outliers/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/handling_outliers/</guid>
      <description>Preliminaries # Load library import pandas as pd Create Data # Create DataFrame houses = pd.DataFrame() houses[&amp;#39;Price&amp;#39;] = [534433, 392333, 293222, 4322032] houses[&amp;#39;Bathrooms&amp;#39;] = [2, 3.5, 2, 116] houses[&amp;#39;Square_Feet&amp;#39;] = [1500, 2500, 1500, 48000] houses   .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Price Bathrooms Square_Feet     0 534433 2.</description>
    </item>
    
    <item>
      <title>Handling Time Zones</title>
      <link>/machine_learning/preprocessing_dates_and_times/handling_time_zones/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/handling_time_zones/</guid>
      <description>Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones[0:10] [&#39;Africa/Abidjan&#39;, &#39;Africa/Accra&#39;, &#39;Africa/Addis_Ababa&#39;, &#39;Africa/Algiers&#39;, &#39;Africa/Asmara&#39;, &#39;Africa/Asmera&#39;, &#39;Africa/Bamako&#39;, &#39;Africa/Bangui&#39;, &#39;Africa/Banjul&#39;, &#39;Africa/Bissau&#39;]  Create Timestamp With Time Zone # Create datetime pd.Timestamp(&amp;#39;2017-05-01 06:00:00&amp;#39;, tz=&amp;#39;Europe/London&amp;#39;) Timestamp(&#39;2017-05-01 06:00:00+0100&#39;, tz=&#39;Europe/London&#39;)  Create Timestamp Without Time Zone # Create datetime date = pd.Timestamp(&amp;#39;2017-05-01 06:00:00&amp;#39;) Add Time Zone # Set time zone date_in_london = date.tz_localize(&amp;#39;Europe/London&amp;#39;) Convert Time Zone # Change time zone date_in_london.</description>
    </item>
    
    <item>
      <title>Harris Corner Detector</title>
      <link>/machine_learning/preprocessing_images/harris_corner_detector/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/harris_corner_detector/</guid>
      <description>The Harris Corner Detector is a commonly used method of detecting the intersection of two edges. It looks for windows (also called neighborhoods or patches) where small movements of the window (imagine shaking the window) creates big changes in the contents of the pixels inside the window.
Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as grayscale image_bgr = cv2.</description>
    </item>
    
    <item>
      <title>Hyperparameter Tuning Using Grid Search</title>
      <link>/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import linear_model, datasets from sklearn.model_selection import GridSearchCV Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Logistic Regression # Create logistic regression logistic = linear_model.LogisticRegression() Create Hyperparameter Search Space # Create regularization penalty space penalty = [&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;] # Create regularization hyperparameter space C = np.logspace(0, 4, 10) # Create hyperparameter options hyperparameters = dict(C=C, penalty=penalty) Create Grid Search # Create grid search using 5-fold cross validation clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0) Conduct Grid Search # Fit grid search best_model = clf.</description>
    </item>
    
    <item>
      <title>Hyperparameter Tuning Using Random Search</title>
      <link>/machine_learning/model_selection/hyperparameter_tuning_using_random_search/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_selection/hyperparameter_tuning_using_random_search/</guid>
      <description>Preliminaries # Load libraries from scipy.stats import uniform from sklearn import linear_model, datasets from sklearn.model_selection import RandomizedSearchCV Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Logistic Regression # Create logistic regression logistic = linear_model.LogisticRegression() Create Hyperparameter Search Space # Create regularization penalty space penalty = [&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;] # Create regularization hyperparameter distribution using uniform distribution C = uniform(loc=0, scale=4) # Create hyperparameter options hyperparameters = dict(C=C, penalty=penalty) Create Random Search # Create randomized search 5-fold cross validation and 100 iterations clf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1) Conduct Random Search # Fit randomized search best_model = clf.</description>
    </item>
    
    <item>
      <title>Identifying Best Value Of k</title>
      <link>/machine_learning/nearest_neighbors/identifying_best_value_of_k/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/nearest_neighbors/identifying_best_value_of_k/</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import KNeighborsClassifier from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.model_selection import GridSearchCV Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Data # Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Fit A k-Nearest Neighbor Classifier # Fit a KNN classifier with 5 neighbors knn = KNeighborsClassifier(n_neighbors=5, metric=&amp;#39;euclidean&amp;#39;, n_jobs=-1).</description>
    </item>
    
    <item>
      <title>Imbalanced Classes In SVM</title>
      <link>/machine_learning/support_vector_machines/imbalanced_classes_in_svm/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/imbalanced_classes_in_svm/</guid>
      <description>In support vector machines, $C$ is a hyperparameter determining the penalty for misclassifying an observation. One method for handling imbalanced classes in support vector machines is to weight $C$ by classes, so that
$$C_k = C * w_j$$
where $C$ is the penalty for misclassification, $w_j$ is a weight inversely proportional to class $j$&amp;rsquo;s frequency, and $C_j$ is the $C$ value for class $j$. The general idea is to increase the penalty for misclassifying minority classes to prevent them from being &amp;ldquo;overwhelmed&amp;rdquo; by the majority class.</description>
    </item>
    
    <item>
      <title>Impute Missing Values With Means</title>
      <link>/machine_learning/preprocessing_structured_data/impute_missing_values_with_means/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/impute_missing_values_with_means/</guid>
      <description>Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most &amp;lsquo;naive&amp;rsquo; imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it.
Preliminaries import pandas as pd import numpy as np from sklearn.preprocessing import Imputer Create Data # Create an empty dataset df = pd.</description>
    </item>
    
    <item>
      <title>Imputing Missing Class Labels</title>
      <link>/machine_learning/preprocessing_structured_data/imputing_missing_class_labels/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/imputing_missing_class_labels/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.preprocessing import Imputer Create Feature Matrix With Missing Values # Create feature matrix with categorical feature X = np.array([[0, 2.10, 1.45], [1, 1.18, 1.33], [0, 1.22, 1.27], [0, -0.21, -1.19], [np.nan, 0.87, 1.31], [np.nan, -0.67, -0.22]]) Fill Missing Values&amp;rsquo; Class With Most Frequent Class # Create Imputer object imputer = Imputer(strategy=&amp;#39;most_frequent&amp;#39;, axis=0) # Fill missing values with most frequent class imputer.</description>
    </item>
    
    <item>
      <title>Imputing Missing Class Labels Using k-Nearest Neighbors</title>
      <link>/machine_learning/preprocessing_structured_data/imputing_missing_class_labels_using_k-nearest_neighbors/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/imputing_missing_class_labels_using_k-nearest_neighbors/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.neighbors import KNeighborsClassifier Create Feature Matrix # Create feature matrix with categorical feature X = np.array([[0, 2.10, 1.45], [1, 1.18, 1.33], [0, 1.22, 1.27], [1, -0.21, -1.19]]) Create Feature Matrix With Missing Values # Create feature matrix with missing values in the categorical feature X_with_nan = np.array([[np.nan, 0.87, 1.31], [np.nan, -0.67, -0.22]]) Train k-Nearest Neighbor Classifier # Train KNN learner clf = KNeighborsClassifier(3, weights=&amp;#39;distance&amp;#39;) trained_model = clf.</description>
    </item>
    
    <item>
      <title>Installing OpenCV</title>
      <link>/machine_learning/preprocessing_images/installing_opencv/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/installing_opencv/</guid>
      <description>While there are a number of good libraries out there, OpenCV is the most popular and documented library for handling images. One of the biggest hurdles to using OpenCV is installing it. However, fortunately we can use Anaconda&amp;rsquo;s package manager tool conda to install OpenCV in a single line of code in our terminal:
conda install --channel https://conda.anaconda.org/menpo opencv3
Afterwards, we can check the installation by opening a notebook, importing OpenCV, and checking the version number (3.</description>
    </item>
    
    <item>
      <title>Invert A Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/invert_a_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/invert_a_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np.array([[1, 4], [2, 5]]) Invert Matrix # Calculate inverse of matrix np.linalg.inv(matrix) array([[-1.66666667, 1.33333333], [ 0.66666667, -0.33333333]])  </description>
    </item>
    
    <item>
      <title>Isolate Colors</title>
      <link>/machine_learning/preprocessing_images/isolate_colors/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/isolate_colors/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;) Convert To HSV Color Format # Convert BGR to HSV image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV) Create Mask # Define range of blue values in HSV lower_blue = np.array([50,100,50]) upper_blue = np.array([130,255,255]) # Create mask mask = cv2.inRange(image_hsv, lower_blue, upper_blue) Apply Mask # Mask image image_bgr_masked = cv2.</description>
    </item>
    
    <item>
      <title>K-Nearest Neighbors Classification</title>
      <link>/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/</guid>
      <description>K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.
KNN has three basic parts:
 $y_i$: The class of an observation (what we are trying to predict in the test data). $X_i$: The predictors/IVs/attributes of an observation. $K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its &amp;ldquo;neighborhood&amp;rdquo;. For example, K=2 means that each observation&amp;rsquo;s has a neighorhood comprising of the two other observations closest to it.</description>
    </item>
    
    <item>
      <title>Lag A Time Feature</title>
      <link>/machine_learning/preprocessing_dates_and_times/lag_a_time_feature/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/lag_a_time_feature/</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date Data # Create data frame df = pd.DataFrame() # Create data df[&amp;#39;dates&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=5, freq=&amp;#39;D&amp;#39;) df[&amp;#39;stock_price&amp;#39;] = [1.1,2.2,3.3,4.4,5.5] Lag Time Data By One Row # Lagged values by one row df[&amp;#39;previous_days_stock_price&amp;#39;] = df[&amp;#39;stock_price&amp;#39;].shift(1) # Show data frame df   .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    dates stock_price previous_days_stock_price     0 2001-01-01 1.</description>
    </item>
    
    <item>
      <title>Lasso Regression</title>
      <link>/machine_learning/linear_regression/lasso_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/lasso_regression/</guid>
      <description>Preliminaries # Load library from sklearn.linear_model import Lasso from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Fit Ridge Regression The hyperparameter, $\alpha$, lets us control how much we penalize the coefficients, with higher values of $\alpha$ creating simpler modelers. The ideal value of $\alpha$ should be tuned like any other hyperparameter.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/machine_learning/linear_regression/linear_regression_scikitlearn/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/linear_regression_scikitlearn/</guid>
      <description>Sources: scikit-learn, DrawMyData.
The purpose of this tutorial is to give a brief introduction into the logic of statistical model building used in machine learning. If you want to read more about the theory behind this tutorial, check out An Introduction To Statistical Learning.
Let us get started.
Preliminary import pandas as pd from sklearn import linear_model import random import numpy as np %matplotlib inline Load Data With those libraries added, let us load the dataset (the dataset is avaliable in his site&amp;rsquo;s GitHub repo).</description>
    </item>
    
    <item>
      <title>Linear Regression Using Scikit-Learn</title>
      <link>/machine_learning/linear_regression/linear_regression_using_scikit-learn/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/linear_regression_using_scikit-learn/</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston import warnings # Suppress Warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Fit A Linear Regression # Create linear regression regr = LinearRegression() # Fit the linear regression model = regr.fit(X, y) View Intercept Term # View the intercept model.intercept_ 36.491103280361038  View Coefficients # View the feature coefficients model.</description>
    </item>
    
    <item>
      <title>Load Images</title>
      <link>/machine_learning/preprocessing_images/load_images/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/load_images/</guid>
      <description>Preliminaries # Load library import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) # Show image plt.imshow(image, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() Load Image As RGB # Load image in color image_bgr = cv2.imread(&amp;#39;images/plane.jpg&amp;#39;, cv2.IMREAD_COLOR) # Convert to RGB image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) # Show image plt.imshow(image_rgb), plt.axis(&amp;#34;off&amp;#34;) plt.show() View Image Data # Show image data image array([[140, 136, 146, .</description>
    </item>
    
    <item>
      <title>Loading Features From Dictionaries</title>
      <link>/machine_learning/basics/loading_features_from_dictionaries/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/loading_features_from_dictionaries/</guid>
      <description>Preliminaries from sklearn.feature_extraction import DictVectorizer Create A Dictionary staff = [{&amp;#39;name&amp;#39;: &amp;#39;Steve Miller&amp;#39;, &amp;#39;age&amp;#39;: 33.}, {&amp;#39;name&amp;#39;: &amp;#39;Lyndon Jones&amp;#39;, &amp;#39;age&amp;#39;: 12.}, {&amp;#39;name&amp;#39;: &amp;#39;Baxter Morth&amp;#39;, &amp;#39;age&amp;#39;: 18.}] Convert Dictionary To Feature Matrix # Create an object for our dictionary vectorizer vec = DictVectorizer()# Fit then transform the staff dictionary with vec, then output an array vec.fit_transform(staff).toarray() array([[ 33., 0., 0., 1.], [ 12., 0., 1., 0.], [ 18., 1., 0., 0.]])  View Feature Names # Get Feature Names vec.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Boston Housing Dataset</title>
      <link>/machine_learning/basics/loading_scikit-learns_boston_housing_dataset/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/loading_scikit-learns_boston_housing_dataset/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt  Load Boston Housing Dataset The Boston housing dataset is a famous dataset from the 1970s. It contains 506 observations on housing prices around Boston. It is often used in regression examples and contains 15 features.
# Load digits dataset boston = datasets.load_boston() # Create feature matrix X = boston.data # Create target vector y = boston.target # View the first observation&amp;#39;s feature values X[0] array([ 6.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Digits Dataset</title>
      <link>/machine_learning/basics/loading_scikit-learns_digits-dataset/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/loading_scikit-learns_digits-dataset/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt  Load Digits Dataset Digits is a dataset of handwritten digits. Each feature is the intensity of one pixel of an 8 x 8 image.
# Load digits dataset digits = datasets.load_digits() # Create feature matrix X = digits.data # Create target vector y = digits.target # View the first observation&amp;#39;s feature values X[0] array([ 0., 0., 5.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn&#39;s Iris Dataset</title>
      <link>/machine_learning/basics/loading_scikit-learns_iris_dataset/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/loading_scikit-learns_iris_dataset/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt  Load Iris Dataset The Iris flower dataset is one of the most famous databases for classification. It contains three classes (i.e. three species of flowers) with 50 observations per class.
# Load digits dataset iris = datasets.load_iris() # Create feature matrix X = iris.data # Create target vector y = iris.target # View the first observation&amp;#39;s feature values X[0] array([ 5.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/machine_learning/logistic_regression/logistic_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/logistic_regression/</guid>
      <description>Despite having &amp;ldquo;regression&amp;rdquo; in its name, a logistic regression is actually a widely used binary classifier (i.e. the target vector can only take two values). In a logistic regression, a linear model (e.g. $\beta_{0}+\beta_{1}x$) is included in a logistic (also called sigmoid) function, ${\frac{1}{1+e^{-z}}}$, such that:
$$P(y_i=1 \mid X)={\frac{1}{1+e^{-(\beta_{0}+\beta_{1}x)}}}$$
where $P(y_i=1 \mid X)$ is the probability of the $i$th observation&amp;rsquo;s target value, $y_i$, being class 1, $X$ is the training data, $\beta_0$ and $\beta_1$ are the parameters to be learned, and $e$ is Euler&amp;rsquo;s number.</description>
    </item>
    
    <item>
      <title>Logistic Regression On Very Large Data</title>
      <link>/machine_learning/logistic_regression/logistic_regression_on_very_large_data/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/logistic_regression_on_very_large_data/</guid>
      <description>scikit-learn&amp;rsquo;s LogisticRegression offers a number of techniques for training a logistic regression, called solvers. Most of the time scikit-learn will select the best solver automatically for us or warn us that you cannot do some thing with that solver. However, there is one particular case we should be aware of.
While an exact explanation is beyond the bounds of this book, stochastic average gradient descent allows us to train a model much faster than other solvers when our data is very large.</description>
    </item>
    
    <item>
      <title>Logistic Regression With L1 Regularization</title>
      <link>/machine_learning/logistic_regression/logistic_regression_with_l1_regularization/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/logistic_regression_with_l1_regularization/</guid>
      <description>L1 regularization (also called least absolute deviations) is a powerful tool in data science. There are many tutorials out there explaining L1 regularization and I will not try to do that here. Instead, this tutorial is show the effect of the regularization parameter C on the coefficients and model accuracy.
Preliminaries import numpy as np from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Create The Data The dataset used in this tutorial is the famous iris dataset.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Classification</title>
      <link>/machine_learning/basics/make_simulated_data_for_classification/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/make_simulated_data_for_classification/</guid>
      <description>Preliminaries from sklearn.datasets import make_classification import pandas as pd Create Simulated Data # Create a simulated feature matrix and output vector with 100 samples, features, output = make_classification(n_samples = 100, # ten features n_features = 10, # five features that actually predict the output&amp;#39;s classes n_informative = 5, # five features that are random and unrelated to the output&amp;#39;s classes n_redundant = 5, # three output classes n_classes = 3, # with 20% of observations in the first class, 30% in the second class, # and 50% in the third class.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Clustering</title>
      <link>/machine_learning/basics/make_simulated_data_for_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/make_simulated_data_for_clustering/</guid>
      <description>Preliminaries from sklearn.datasets import make_blobs import matplotlib.pyplot as plt Make Data # Make the features (X) and output (y) with 200 samples, X, y = make_blobs(n_samples = 200, # two feature variables, n_features = 2, # three clusters, centers = 3, # with .5 cluster standard deviation, cluster_std = 0.5, # shuffled, shuffle = True) View Data # Create a scatterplot of the first and second features plt.scatter(X[:,0], X[:,1]) # Show the scatterplot plt.</description>
    </item>
    
    <item>
      <title>Make Simulated Data For Regression</title>
      <link>/machine_learning/basics/make_simulated_data_for_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/make_simulated_data_for_regression/</guid>
      <description>Preliminaries import pandas as pd from sklearn.datasets import make_regression Create Simulated Data # Generate fetures, outputs, and true coefficient of 100 samples, features, output, coef = make_regression(n_samples = 100, # three features n_features = 3, # where only two features are useful, n_informative = 2, # a single target value per observation n_targets = 1, # 0.0 standard deviation of the guassian noise noise = 0.0, # show the true coefficient used to generated the data coef = True) View Simulated Data # View the features of the first five rows pd.</description>
    </item>
    
    <item>
      <title>Meanshift Clustering</title>
      <link>/machine_learning/clustering/meanshift_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/meanshift_clustering/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import MeanShift Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct Meanshift Clustering MeanShift has two important parameters we should be aware of. First, bandwidth sets radius of the area (i.e. kernel) an observation uses to determine the direction to shift.</description>
    </item>
    
    <item>
      <title>Mini-Batch k-Means Clustering</title>
      <link>/machine_learning/clustering/minibatch_k-means_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/minibatch_k-means_clustering/</guid>
      <description>Mini-batch k-means works similarly to the k-means algorithm discussed in the last recipe. Without going into too much detail, the difference is that in mini-batch k-means the most computationally costly step is conducted on only a random sample of observations as opposed to all observations. This approach can significantly reduce the time required for the algorithm to find convergence (i.e. fit the data) with only a small cost in quality.</description>
    </item>
    
    <item>
      <title>Model Selection Using Grid Search</title>
      <link>/machine_learning/model_selection/model_selection_using_grid_search/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_selection/model_selection_using_grid_search/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline # Set random seed np.random.seed(0) Load Iris Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Pipeline With Model Selection Search Space Notice that we include both multiple possible learning algorithms and multiple possible hyperparameter values to search over.</description>
    </item>
    
    <item>
      <title>Multinomial Logistic Regression</title>
      <link>/machine_learning/naive_bayes/multinomial_logistic_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/multinomial_logistic_regression/</guid>
      <description>In multinomial logistic regression (MLR) the logistic function we saw in Recipe 15.1 is replaced with a softmax function:
$$P(y_i=k \mid X)={\frac {e^{\beta_{k}x_{i}}}{{\sum_{j=1}^{K}}e^{\beta_{j}x_{i}}}}$$
where $P(y_i=k \mid X)$ is the probability the $i$th observation&amp;rsquo;s target value, $y_i$, is class $k$, and $K$ is the total number of classes. One practical advantage of the MLR is that its predicted probabilities using the predict_proba method are more reliable (i.e. better calibrated).
Preliminaries # Load libraries from sklearn.</description>
    </item>
    
    <item>
      <title>Multinomial Naive Bayes Classifier</title>
      <link>/machine_learning/naive_bayes/multinomial_naive_bayes_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/multinomial_naive_bayes_classifier/</guid>
      <description>Multinomial naive Bayes works similar to Gaussian naive Bayes, however the features are assumed to be multinomially distributed. In practice, this means that this classifier is commonly used when we have discrete data (e.g. movie ratings ranging 1 and 5).
Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Brazil is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Bag Of Words # Create bag of words count = CountVectorizer() bag_of_words = count.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier From Scratch</title>
      <link>/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/</guid>
      <description>Naive bayes is simple classifier known for doing well when only a small number of observations is available. In this tutorial we will create a gaussian naive bayes classifier from scratch and use it to predict the class of a previously unseen data point. This tutorial is based on an example on Wikipedia&amp;rsquo;s naive bayes classifier page, I have implemented it in Python and tweaked some notation to improve explanation.</description>
    </item>
    
    <item>
      <title>Nested Cross Validation</title>
      <link>/machine_learning/model_evaluation/nested_cross_validation/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/nested_cross_validation/</guid>
      <description>Often we want to tune the parameters of a model (for example, C in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation:
 Set the parameter you want to tune to some value. Split your data into K &amp;lsquo;folds&amp;rsquo; (sections). Train your model using K-1 folds using the parameter value.</description>
    </item>
    
    <item>
      <title>Normalizing Observations</title>
      <link>/machine_learning/preprocessing_structured_data/normalizing_observations/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/normalizing_observations/</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import Normalizer import numpy as np Create Feature Matrix # Create feature matrix X = np.array([[0.5, 0.5], [1.1, 3.4], [1.5, 20.2], [1.63, 34.4], [10.9, 3.3]]) Normalize Observations Normalizer rescales the values on individual observations to have unit norm (the sum of their lengths is one).
# Create normalizer normalizer = Normalizer(norm=&amp;#39;l2&amp;#39;) # Transform feature matrix normalizer.transform(X) array([[ 0.70710678, 0.70710678], [ 0.30782029, 0.95144452], [ 0.</description>
    </item>
    
    <item>
      <title>One Vs. Rest Logistic Regression</title>
      <link>/machine_learning/logistic_regression/one-vs-rest_logistic_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/logistic_regression/one-vs-rest_logistic_regression/</guid>
      <description>On their own, logistic regressions are only binary classifiers, meaning they cannot handle target vectors with more than two classes. However, there are clever extensions to logistic regression to do just that. In one-vs-rest logistic regression (OVR) a separate model is trained for each class predicted whether an observation is that class or not (thus making it a binary classification problem). It assumes that each classification problem (e.g. class 0 or not) is independent.</description>
    </item>
    
    <item>
      <title>One-Hot Encode Features With Multiple Labels</title>
      <link>/machine_learning/preprocessing_structured_data/one-hot_encode_features_with_multiple_labels/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/one-hot_encode_features_with_multiple_labels/</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import MultiLabelBinarizer import numpy as np Create Data # Create NumPy array y = [(&amp;#39;Texas&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;California&amp;#39;, &amp;#39;Alabama&amp;#39;), (&amp;#39;Texas&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;Delware&amp;#39;, &amp;#39;Florida&amp;#39;), (&amp;#39;Texas&amp;#39;, &amp;#39;Alabama&amp;#39;)] One-hot Encode Data # Create MultiLabelBinarizer object one_hot = MultiLabelBinarizer() # One-hot encode data one_hot.fit_transform(y) array([[0, 0, 0, 1, 1], [1, 1, 0, 0, 0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 0], [1, 0, 0, 0, 1]])  View Column Headers # View classes one_hot.</description>
    </item>
    
    <item>
      <title>One-Hot Encode Nominal Categorical Features</title>
      <link>/machine_learning/preprocessing_structured_data/one-hot_encode_nominal_categorical_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/one-hot_encode_nominal_categorical_features/</guid>
      <description>Preliminaries # Load libraries import numpy as np import pandas as pd from sklearn.preprocessing import LabelBinarizer Create Data With One Class Label # Create NumPy array x = np.array([[&amp;#39;Texas&amp;#39;], [&amp;#39;California&amp;#39;], [&amp;#39;Texas&amp;#39;], [&amp;#39;Delaware&amp;#39;], [&amp;#39;Texas&amp;#39;]]) One-hot Encode Data (Method 1) # Create LabelBinzarizer object one_hot = LabelBinarizer() # One-hot encode data one_hot.fit_transform(x) array([[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1]])  View Column Headers # View classes one_hot.</description>
    </item>
    
    <item>
      <title>Parse HTML</title>
      <link>/machine_learning/preprocessing_text/parse_html/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/parse_html/</guid>
      <description> Preliminaries # Load library from bs4 import BeautifulSoup Create HTML # Create some HTML code html = &amp;#34;&amp;lt;div class=&amp;#39;full_name&amp;#39;&amp;gt;&amp;lt;span style=&amp;#39;font-weight:bold&amp;#39;&amp;gt;Masego&amp;lt;/span&amp;gt; Azra&amp;lt;/div&amp;gt;&amp;#34; Parse HTML # Parse html soup = BeautifulSoup(html, &amp;#34;lxml&amp;#34;) # Find the div with the class &amp;#34;full_name&amp;#34;, show text soup.find(&amp;#34;div&amp;#34;, { &amp;#34;class&amp;#34; : &amp;#34;full_name&amp;#34; }).text &#39;Masego Azra&#39;  </description>
    </item>
    
    <item>
      <title>Perceptron In Scikit</title>
      <link>/machine_learning/basics/perceptron_in_scikit-learn/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/perceptron_in_scikit-learn/</guid>
      <description>A perceptron learner was one of the earliest machine learning techniques and still from the foundation of many modern neural networks. In this tutorial we use a perceptron learner to classify the famous iris dataset. This tutorial was inspired by Python Machine Learning by Sebastian Raschka.
Preliminaries # Load required libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Perceptron from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np Load The Iris Data # Load the iris dataset iris = datasets.</description>
    </item>
    
    <item>
      <title>Pipelines With Parameter Optimization</title>
      <link>/machine_learning/model_selection/pipelines_with_parameter_optimization/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_selection/pipelines_with_parameter_optimization/</guid>
      <description>Preliminaries # Import required packages import numpy as np from sklearn import linear_model, decomposition, datasets from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.preprocessing import StandardScaler Load Data # Load the breast cancer data dataset = datasets.load_breast_cancer() # Create X from the dataset&amp;#39;s features X = dataset.data # Create y from the dataset&amp;#39;s output y = dataset.target Create Pipelines # Create an scaler object sc = StandardScaler() # Create a pca object pca = decomposition.</description>
    </item>
    
    <item>
      <title>Plot The Learning Curve</title>
      <link>/machine_learning/model_evaluation/plot_the_learning_curve/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/plot_the_learning_curve/</guid>
      <description>Preliminaries # Load libraries import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve Load Digits Dataset # Load data digits = load_digits() # Create feature matrix and target vector X, y = digits.data, digits.target Plot Learning Curve # Create CV training and test scores for various training set sizes train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), X, y, # Number of folds in cross-validation cv=10, # Evaluation metric scoring=&amp;#39;accuracy&amp;#39;, # Use all computer cores n_jobs=-1, # 50 different sizes of the training set train_sizes=np.</description>
    </item>
    
    <item>
      <title>Plot The Receiving Operating Characteristic Curve</title>
      <link>/machine_learning/model_evaluation/plot_the_receiving_operating_characteristic_curve/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/plot_the_receiving_operating_characteristic_curve/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve, roc_auc_score from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt Generate Features And Target # Create feature matrix and target vector X, y = make_classification(n_samples=10000, n_features=10, n_classes=2, n_informative=3, random_state=3) Split Data Intro Training And Test Sets # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1) Training Binary Classifier # Create classifier clf = LogisticRegression() # Train model clf.</description>
    </item>
    
    <item>
      <title>Plot The Support Vector Classifiers Hyperplane</title>
      <link>/machine_learning/support_vector_machines/plot_support_vector_classifier_hyperplane/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/plot_support_vector_classifier_hyperplane/</guid>
      <description>Preliminaries # Load libraries from sklearn.svm import LinearSVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np from matplotlib import pyplot as plt Load Iris Flower Data # Load data with only two classes and two features iris = datasets.load_iris() X = iris.data[:100,:2] y = iris.target[:100] Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Train Support Vector Classifier # Create support vector classifier svc = LinearSVC(C=1.</description>
    </item>
    
    <item>
      <title>Plot The Validation Curve</title>
      <link>/machine_learning/model_evaluation/plot_the_validation_curve/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/plot_the_validation_curve/</guid>
      <description>Preliminaries # Load libraries import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import validation_curve Load Digits Dataset # Load data digits = load_digits() # Create feature matrix and target vector X, y = digits.data, digits.target Plot Validation Curve # Create range of values for parameter param_range = np.arange(1, 250, 2) # Calculate accuracy on training and test set using range of parameter values train_scores, test_scores = validation_curve(RandomForestClassifier(), X, y, param_name=&amp;#34;n_estimators&amp;#34;, param_range=param_range, cv=3, scoring=&amp;#34;accuracy&amp;#34;, n_jobs=-1) # Calculate mean and standard deviation for training set scores train_mean = np.</description>
    </item>
    
    <item>
      <title>Precision</title>
      <link>/machine_learning/model_evaluation/precision/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/precision/</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Precision # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;precision&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Preprocessing Categorical Features</title>
      <link>/machine_learning/preprocessing_structured_data/preprocessing_categorical_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/preprocessing_categorical_features/</guid>
      <description>Often, machine learning methods (e.g. logistic regression, SVM with a linear kernel, etc) will require that categorical variables be converted into dummy variables (also called OneHot encoding). For example, a single feature Fruit would be converted into three features, Apples, Oranges, and Bananas, one for each category in the categorical feature.
There are common ways to preprocess categorical features: using pandas or scikit-learn.
Preliminaries from sklearn import preprocessing from sklearn.</description>
    </item>
    
    <item>
      <title>Preprocessing Iris Data</title>
      <link>/machine_learning/preprocessing_structured_data/preprocessing_iris_data/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/preprocessing_iris_data/</guid>
      <description>Preliminaries from sklearn import datasets import numpy as np from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler Load Data # Load the iris data iris = datasets.load_iris() # Create a variable for the feature data X = iris.data # Create a variable for the target data y = iris.target Split Data For Cross Validation # Random split the data into four new datasets, training features, training outcome, test features,  # and test outcome.</description>
    </item>
    
    <item>
      <title>Radius-Based Nearest Neighbor Classifier</title>
      <link>/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier/</guid>
      <description>Preliminaries # Load libraries from sklearn.neighbors import RadiusNeighborsClassifier from sklearn.preprocessing import StandardScaler from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data y = iris.target Standardize Features # Create standardizer standardizer = StandardScaler() # Standardize features X_std = standardizer.fit_transform(X) Fit A Radius-Based Nearest Neighbor Classifier In scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius.</description>
    </item>
    
    <item>
      <title>Random Forest Classifier</title>
      <link>/machine_learning/trees_and_forests/random_forest_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/random_forest_classifier/</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets Load Iris Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Random Forest Classifier # Create random forest classifer object that uses entropy clf = RandomForestClassifier(criterion=&amp;#39;entropy&amp;#39;, random_state=0, n_jobs=-1) Train Random Forest Classifier # Train model model = clf.fit(X, y) Predict Previously Unseen Observation # Make new observation observation = [[ 5, 4, 3, 2]] # Predict observation&amp;#39;s class  model.</description>
    </item>
    
    <item>
      <title>Random Forest Classifier Example</title>
      <link>/machine_learning/trees_and_forests/random_forest_classifier_example/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/random_forest_classifier_example/</guid>
      <description>This tutorial is based on Yhat&amp;rsquo;s 2013 tutorial on Random Forests in Python. If you want a good summary of the theory and uses of random forests, I suggest you check out their guide. In the tutorial below, I annotate, correct, and expand on a short code example of random forests they present at the end of the article. Specifically, I 1) update the code so it runs in the latest version of pandas and Python, 2) write detailed comments explaining what is happening in each step, and 3) expand the code in a number of ways.</description>
    </item>
    
    <item>
      <title>Random Forest Regression</title>
      <link>/machine_learning/trees_and_forests/random_forest_regressor/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/random_forest_regressor/</guid>
      <description> Preliminaries # Load libraries from sklearn.ensemble import RandomForestRegressor from sklearn import datasets Load Boston Housing Data # Load data with only two features boston = datasets.load_boston() X = boston.data[:,0:2] y = boston.target Create Random Forest Regressor # Create decision tree classifer object regr = RandomForestRegressor(random_state=0, n_jobs=-1) Train Random Forest Regressor # Train model model = regr.fit(X, y)</description>
    </item>
    
    <item>
      <title>Recall</title>
      <link>/machine_learning/model_evaluation/recall/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/recall/</guid>
      <description>Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X, y = make_classification(n_samples = 10000, n_features = 3, n_informative = 3, n_redundant = 0, n_classes = 2, random_state = 1) Create Logistic Regression # Create logistic regression logit = LogisticRegression() Cross-Validate Model Using Recall # Cross-validate model using precision cross_val_score(logit, X, y, scoring=&amp;#34;recall&amp;#34;) array([ 0.</description>
    </item>
    
    <item>
      <title>Recursive Feature Elimination</title>
      <link>/machine_learning/feature_selection/recursive_feature_elimination/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/recursive_feature_elimination/</guid>
      <description>Preliminaries # Load libraries from sklearn.datasets import make_regression from sklearn.feature_selection import RFECV from sklearn import datasets, linear_model import warnings # Suppress an annoying but harmless warning warnings.filterwarnings(action=&amp;#34;ignore&amp;#34;, module=&amp;#34;scipy&amp;#34;, message=&amp;#34;^internal gelsd&amp;#34;) Create Data # Generate features matrix, target vector, and the true coefficients X, y = make_regression(n_samples = 10000, n_features = 100, n_informative = 2, random_state = 1) Create Linear Model # Create a linear regression ols = linear_model.LinearRegression() Conduct Recursive Feature Elimination # Create recursive feature eliminator that scores features by mean squared errors rfecv = RFECV(estimator=ols, step=1, scoring=&amp;#39;neg_mean_squared_error&amp;#39;) # Fit recursive feature eliminator  rfecv.</description>
    </item>
    
    <item>
      <title>Remove Backgrounds</title>
      <link>/machine_learning/preprocessing_images/remove_backgrounds/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/remove_backgrounds/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;) Convert To RGB # Convert to RGB image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) Draw Rectangle Around Foreground # Rectange values: start x, start y, width, height rectangle = (0, 56, 256, 150) Apply GrabCut # Create initial mask mask = np.zeros(image_rgb.shape[:2], np.uint8) # Create temporary arrays used by grabCut bgdModel = np.</description>
    </item>
    
    <item>
      <title>Remove Punctuation</title>
      <link>/machine_learning/preprocessing_text/remove_punctuation/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/remove_punctuation/</guid>
      <description> Preliminaries # Load libraries import string import numpy as np Create Text Data # Create text text_data = [&amp;#39;Hi!!!! I. Love. This. Song....&amp;#39;, &amp;#39;10000% Agree!!!! #LoveIT&amp;#39;, &amp;#39;Right?!?!&amp;#39;] Remove Punctuation # Create function using string.punctuation to remove all punctuation def remove_punctuation(sentence: str) -&amp;gt; str: return sentence.translate(str.maketrans(&amp;#39;&amp;#39;, &amp;#39;&amp;#39;, string.punctuation)) # Apply function [remove_punctuation(sentence) for sentence in text_data] [&#39;Hi I Love This Song&#39;, &#39;10000 Agree LoveIT&#39;, &#39;Right&#39;]  </description>
    </item>
    
    <item>
      <title>Remove Stop Words</title>
      <link>/machine_learning/preprocessing_text/remove_stop_words/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/remove_stop_words/</guid>
      <description> Preliminaries # Load library from nltk.corpus import stopwords # You will have to download the set of stop words the first time import nltk nltk.download(&amp;#39;stopwords&amp;#39;) [nltk_data] Downloading package stopwords to [nltk_data] /Users/chrisalbon/nltk_data... [nltk_data] Package stopwords is already up-to-date! True  Create Word Tokens # Create word tokens tokenized_words = [&amp;#39;i&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;going&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;go&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;store&amp;#39;, &amp;#39;and&amp;#39;, &amp;#39;park&amp;#39;] Load Stop Words # Load stop words stop_words = stopwords.words(&amp;#39;english&amp;#39;) # Show stop words stop_words[:5] [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;]  Remove Stop Words # Remove stop words [word for word in tokenized_words if word not in stop_words] [&#39;going&#39;, &#39;go&#39;, &#39;store&#39;, &#39;park&#39;]  </description>
    </item>
    
    <item>
      <title>Replace Characters</title>
      <link>/machine_learning/preprocessing_text/replace_characters/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/replace_characters/</guid>
      <description>Preliminaries # Import library import re Create Text # Create text text_data = [&amp;#39;Interrobang. By Aishwarya Henriette&amp;#39;, &amp;#39;Parking And Going. By Karl Gautier&amp;#39;, &amp;#39;Today Is The night. By Jarek Prakash&amp;#39;] Replace Character (Method 1) # Remove periods remove_periods = [string.replace(&amp;#39;.&amp;#39;, &amp;#39;&amp;#39;) for string in text_data] # Show text remove_periods [&#39;Interrobang By Aishwarya Henriette&#39;, &#39;Parking And Going By Karl Gautier&#39;, &#39;Today Is The night By Jarek Prakash&#39;]  Replace Character (Method 2) # Create function def replace_letters_with_X(string: str) -&amp;gt; str: return re.</description>
    </item>
    
    <item>
      <title>Rescale A Feature</title>
      <link>/machine_learning/preprocessing_structured_data/rescale_a_feature/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/rescale_a_feature/</guid>
      <description> 
Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]]) Rescale Feature Using Min-Max # Create scaler minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1)) # Scale feature x_scale = minmax_scale.fit_transform(x) # Show feature x_scale array([[ 0. ], [ 0.28571429], [ 0.35714286], [ 0.42857143], [ 1. ]])  </description>
    </item>
    
    <item>
      <title>Reshape An Array</title>
      <link>/machine_learning/vectors_matrices_and_arrays/reshape_an_array/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/reshape_an_array/</guid>
      <description> Preliminaries # Load library import numpy as np Create Array # Create a 4x3 matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) Reshape Array # Reshape matrix into 2x6 matrix matrix.reshape(2, 6) array([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]])  </description>
    </item>
    
    <item>
      <title>Ridge Regression</title>
      <link>/machine_learning/linear_regression/ridge_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/ridge_regression/</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import Ridge from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Fit Ridge Regression The hyperparameter, $\alpha$, lets us control how much we penalize the coefficients, with higher values of $\alpha$ creating simpler modelers. The ideal value of $\alpha$ should be tuned like any other hyperparameter.</description>
    </item>
    
    <item>
      <title>Rolling Time Window</title>
      <link>/machine_learning/preprocessing_dates_and_times/rolling_time_windows/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/rolling_time_windows/</guid>
      <description>Preliminaries # Load library import pandas as pd Create Date Data # Create datetimes time_index = pd.date_range(&amp;#39;01/01/2010&amp;#39;, periods=5, freq=&amp;#39;M&amp;#39;) # Create data frame, set index df = pd.DataFrame(index=time_index) # Create feature df[&amp;#39;Stock_Price&amp;#39;] = [1,2,3,4,5] Create A Rolling Time Window Of Two Rows # Calculate rolling mean df.rolling(window=2).mean()   .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Stock_Price     2010-01-31 NaN   2010-02-28 1.</description>
    </item>
    
    <item>
      <title>SVC Parameters When Using RBF Kernel</title>
      <link>/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/</guid>
      <description>In this tutorial we will visually explore the effects of the two parameters from the support vector classifier (SVC) when using the radial basis function kernel (RBF). This tutorial draws heavily on the code used in Sebastian Raschka&amp;rsquo;s book Python Machine Learning.
Preliminaries # Import packages to visualize the classifer from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt import warnings # Import packages to do the classifying import numpy as np from sklearn.</description>
    </item>
    
    <item>
      <title>Save Images</title>
      <link>/machine_learning/preprocessing_images/save_images/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/save_images/</guid>
      <description> Preliminaries # Load library import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) # Show image plt.imshow(image, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() Save Image # Save image cv2.imwrite(&amp;#39;images/plane_new.jpg&amp;#39;, image) True  </description>
    </item>
    
    <item>
      <title>Saving Machine Learning Models</title>
      <link>/machine_learning/basics/saving_machine_learning_models/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/basics/saving_machine_learning_models/</guid>
      <description>In scikit there are two main ways to save a model for future use: a pickle string and a pickled model as a file.
Preliminaries from sklearn.linear_model import LogisticRegression from sklearn import datasets import pickle from sklearn.externals import joblib Load Data # Load the iris data iris = datasets.load_iris() # Create a matrix, X, of features and a vector, y. X, y = iris.data, iris.target Train Model # Train a naive logistic regression model clf = LogisticRegression(random_state=0) clf.</description>
    </item>
    
    <item>
      <title>Select Date And Time Ranges</title>
      <link>/machine_learning/preprocessing_dates_and_times/select_date_and_time_ranges/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_dates_and_times/select_date_and_time_ranges/</guid>
      <description>Preliminaries # Load library import pandas as pd Create pandas Series Time Data # Create data frame df = pd.DataFrame() # Create datetimes df[&amp;#39;date&amp;#39;] = pd.date_range(&amp;#39;1/1/2001&amp;#39;, periods=100000, freq=&amp;#39;H&amp;#39;) Select Time Range (Method 1) Use this method if your data frame is not indexed by time.
# Select observations between two datetimes df[(df[&amp;#39;date&amp;#39;] &amp;gt; &amp;#39;2002-1-1 01:00:00&amp;#39;) &amp;amp; (df[&amp;#39;date&amp;#39;] &amp;lt;= &amp;#39;2002-1-1 04:00:00&amp;#39;)]   .dataframe thead tr:only-child th { text-align: right; } .</description>
    </item>
    
    <item>
      <title>Select Important Features In Random Forest</title>
      <link>/machine_learning/trees_and_forests/select_important_features_in_random_forest/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/select_important_features_in_random_forest/</guid>
      <description>Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets from sklearn.feature_selection import SelectFromModel Load Iris Flower Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Create Random Forest Classifier # Create random forest classifier clf = RandomForestClassifier(random_state=0, n_jobs=-1) Select Features With Importance Greater Than Threshold The higher the number, the more important the feature (all importance scores sum to one). By plotting these values we can add interpretability to our random forest models.</description>
    </item>
    
    <item>
      <title>Selecting Elements In An Array</title>
      <link>/machine_learning/vectors_matrices_and_arrays/selecting_elements_in_an_array/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/selecting_elements_in_an_array/</guid>
      <description> Preliminaries # Load library import numpy as np Create Vector # Create row vector vector = np.array([1, 2, 3, 4, 5, 6]) Select Element # Select second element vector[1] 2  Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Select Element # Select second row, second column matrix[1,1] 5  Create Tensor # Create matrix tensor = np.array([ [[[1, 1], [1, 1]], [[2, 2], [2, 2]]], [[[3, 3], [3, 3]], [[4, 4], [4, 4]]] ]) Select Element # Select second element of each of the three dimensions tensor[1,1,1] array([4, 4])  </description>
    </item>
    
    <item>
      <title>Selecting The Best Alpha Value In Ridge Regression</title>
      <link>/machine_learning/linear_regression/selecting_best_alpha_value_in_ridge_regression/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/linear_regression/selecting_best_alpha_value_in_ridge_regression/</guid>
      <description>Preliminaries # Load libraries from sklearn.linear_model import RidgeCV from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston() X = boston.data y = boston.target Standardize Features Note: Because in linear regression the value of the coefficients is partially determined by the scale of the feature, and in regularized models all coefficients are summed together, we must make sure to standardize the feature prior to training.</description>
    </item>
    
    <item>
      <title>Selecting The Best Number Of Components For LDA</title>
      <link>/machine_learning/feature_engineering/select_best_number_of_components_in_lda/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/select_best_number_of_components_in_lda/</guid>
      <description>In scikit-learn, LDA is implemented using LinearDiscriminantAnalysis includes a parameter, n_components indicating the number of features we want returned. To figure out what argument value to use with n_components (e.g. how many parameters to keep), we can take advantage of the fact that explained_variance_ratio_ tells us the variance explained by each outputted feature and is a sorted array.
Specifically, we can run LinearDiscriminantAnalysis with n_components set to None to return ratio of variance explained by every component feature, then calculate how many components are required to get above some threshold of variance explained (often 0.</description>
    </item>
    
    <item>
      <title>Selecting The Best Number Of Components For TSVD</title>
      <link>/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/</guid>
      <description>Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets.load_digits() # Standardize the feature matrix X = StandardScaler().fit_transform(digits.data) # Make sparse matrix X_sparse = csr_matrix(X) Run Truncated Singular Value Decomposition # Create and run an TSVD with one less than number of features tsvd = TruncatedSVD(n_components=X_sparse.</description>
    </item>
    
    <item>
      <title>Sharpen Images</title>
      <link>/machine_learning/preprocessing_images/sharpen_images/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/sharpen_images/</guid>
      <description> Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_GRAYSCALE) Sharpen Image # Create kernel kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]]) # Sharpen image image_sharp = cv2.filter2D(image, -1, kernel) View Image # Show image plt.imshow(image_sharp, cmap=&amp;#39;gray&amp;#39;), plt.axis(&amp;#34;off&amp;#34;) plt.show() </description>
    </item>
    
    <item>
      <title>Shi-Tomasi Corner Detector</title>
      <link>/machine_learning/preprocessing_images/ski-tomasi_corner_detector/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/ski-tomasi_corner_detector/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load images image_bgr = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;) image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY) Define Corner Parameters # Number of corners to detect corners_to_detect = 10 minimum_quality_score = 0.05 minimum_distance = 25 Detect Corners # Detect corners corners = cv2.goodFeaturesToTrack(image_gray, corners_to_detect, minimum_quality_score, minimum_distance) corners = np.float32(corners) Mark Corners # Draw white circle at each corner for corner in corners: x, y = corner[0] cv2.</description>
    </item>
    
    <item>
      <title>Split Data Into Training And Test Sets</title>
      <link>/machine_learning/model_evaluation/split_data_into_training_and_test_sets/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/model_evaluation/split_data_into_training_and_test_sets/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split Load Digits Dataset # Load the digits dataset digits = datasets.load_digits() # Create the features matrix X = digits.data # Create the target vector y = digits.target Split Into Training And Test Sets # Create training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1) Fit Standardizer To Training Set # Create standardizer standardizer = StandardScaler() # Fit standardizer to training set standardizer.</description>
    </item>
    
    <item>
      <title>Standardize A Feature</title>
      <link>/machine_learning/preprocessing_structured_data/standardize_a_feature/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_structured_data/standardize_a_feature/</guid>
      <description> 
Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]]) Standardize Feature # Create scaler scaler = preprocessing.StandardScaler() # Transform the feature standardized = scaler.fit_transform(x) # Show feature standardized array([[-1.26687088], [-0.39316683], [-0.17474081], [ 0.0436852 ], [ 1.79109332]])  </description>
    </item>
    
    <item>
      <title>Stemming Words</title>
      <link>/machine_learning/preprocessing_text/stemming_words/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/stemming_words/</guid>
      <description> 
Preliminaries # Load library from nltk.stem.porter import PorterStemmer Create Text Data # Create word tokens tokenized_words = [&amp;#39;i&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;humbled&amp;#39;, &amp;#39;by&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;traditional&amp;#39;, &amp;#39;meeting&amp;#39;] Stem Words Stemming reduces a word to its stem by identifying and removing affixes (e.g. gerunds) while keeping the root meaning of the word. NLTK&amp;rsquo;s PorterStemmer implements the widely used Porter stemming algorithm.
# Create stemmer porter = PorterStemmer() # Apply stemmer [porter.stem(word) for word in tokenized_words] [&#39;i&#39;, &#39;am&#39;, &#39;humbl&#39;, &#39;by&#39;, &#39;thi&#39;, &#39;tradit&#39;, &#39;meet&#39;]  </description>
    </item>
    
    <item>
      <title>Strip Whitespace</title>
      <link>/machine_learning/preprocessing_text/strip_whitespace/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/strip_whitespace/</guid>
      <description> Create Text # Create text text_data = [&amp;#39; Interrobang. By Aishwarya Henriette &amp;#39;, &amp;#39;Parking And Going. By Karl Gautier&amp;#39;, &amp;#39; Today Is The night. By Jarek Prakash &amp;#39;] Remove Whitespace # Strip whitespaces strip_whitespace = [string.strip() for string in text_data] # Show text strip_whitespace [&#39;Interrobang. By Aishwarya Henriette&#39;, &#39;Parking And Going. By Karl Gautier&#39;, &#39;Today Is The night. By Jarek Prakash&#39;]  </description>
    </item>
    
    <item>
      <title>Support Vector Classifier</title>
      <link>/machine_learning/support_vector_machines/support_vector_classifier/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/support_vector_machines/support_vector_classifier/</guid>
      <description>There is a balance between SVC maximizing the margin of the hyperplane and minimizing the misclassification. In SVC, the later is controlled with the hyperparameter $C$, the penalty imposed on errors. C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias but low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias but high variance).</description>
    </item>
    
    <item>
      <title>Tag Parts Of Speech</title>
      <link>/machine_learning/preprocessing_text/tag_parts_of_speech/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/tag_parts_of_speech/</guid>
      <description>Preliminaries # Load libraries from nltk import pos_tag from nltk import word_tokenize Create Text Data # Create text text_data = &amp;#34;Chris loved outdoor running&amp;#34; Tag Parts Of Speech # Use pre-trained part of speech tagger text_tagged = pos_tag(word_tokenize(text_data)) # Show parts of speech text_tagged [(&#39;Chris&#39;, &#39;NNP&#39;), (&#39;loved&#39;, &#39;VBD&#39;), (&#39;outdoor&#39;, &#39;RP&#39;), (&#39;running&#39;, &#39;VBG&#39;)]  Common Penn Treebank Parts Of Speech Tags The output is a list of tuples with the word and the tag of the part of speech.</description>
    </item>
    
    <item>
      <title>Term Frequency Inverse Document Frequency</title>
      <link>/machine_learning/preprocessing_text/tf-idf/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/tf-idf/</guid>
      <description>Preliminaries # Load libraries import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer import pandas as pd Create Text Data # Create text text_data = np.array([&amp;#39;I love Brazil. Brazil!&amp;#39;, &amp;#39;Sweden is best&amp;#39;, &amp;#39;Germany beats both&amp;#39;]) Create Feature Matrix # Create the tf-idf feature matrix tfidf = TfidfVectorizer() feature_matrix = tfidf.fit_transform(text_data) # Show tf-idf feature matrix feature_matrix.toarray() array([[ 0. , 0. , 0. , 0.89442719, 0. , 0. , 0.4472136 , 0.</description>
    </item>
    
    <item>
      <title>Titanic Competition With Random Forest</title>
      <link>/machine_learning/trees_and_forests/titanic_competition_with_random_forest/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/titanic_competition_with_random_forest/</guid>
      <description>Preliminaries import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV, cross_val_score import csv as csv Get The Data You can get the data on Kaggle&amp;rsquo;s site.
# Load the data train = pd.read_csv(&amp;#39;data/train.csv&amp;#39;) test = pd.read_csv(&amp;#39;data/test.csv&amp;#39;) Data Cleaning # Create a list of the features we will eventually want for our model features = [&amp;#39;Age&amp;#39;, &amp;#39;SibSp&amp;#39;,&amp;#39;Parch&amp;#39;,&amp;#39;Fare&amp;#39;,&amp;#39;male&amp;#39;,&amp;#39;embarked_Q&amp;#39;,&amp;#39;embarked_S&amp;#39;,&amp;#39;Pclass_2&amp;#39;, &amp;#39;Pclass_3&amp;#39;] Sex Here we convert the gender labels (male, female) into a dummy variable (1, 0).</description>
    </item>
    
    <item>
      <title>Tokenize Text</title>
      <link>/machine_learning/preprocessing_text/tokenize_text/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_text/tokenize_text/</guid>
      <description> Preliminaries # Load library from nltk.tokenize import word_tokenize, sent_tokenize Create Text Data # Create text string = &amp;#34;The science of today is the technology of tomorrow. Tomorrow is today.&amp;#34; Tokenize Words # Tokenize words word_tokenize(string) [&#39;The&#39;, &#39;science&#39;, &#39;of&#39;, &#39;today&#39;, &#39;is&#39;, &#39;the&#39;, &#39;technology&#39;, &#39;of&#39;, &#39;tomorrow&#39;, &#39;.&#39;, &#39;Tomorrow&#39;, &#39;is&#39;, &#39;today&#39;, &#39;.&#39;]  Tokenize Sentences # Tokenize sentences sent_tokenize(string) [&#39;The science of today is the technology of tomorrow.&#39;, &#39;Tomorrow is today.&#39;]  </description>
    </item>
    
    <item>
      <title>Transpose A Vector Or Matrix</title>
      <link>/machine_learning/vectors_matrices_and_arrays/transpose_a_vector_or_matrix/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/vectors_matrices_and_arrays/transpose_a_vector_or_matrix/</guid>
      <description> Preliminaries # Load library import numpy as np Create Vector # Create vector vector = np.array([1, 2, 3, 4, 5, 6]) Create Matrix # Create matrix matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Transpose Vector # Tranpose vector vector.T array([1, 2, 3, 4, 5, 6])  Transpose Matrix # Transpose matrix matrix.T array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])  </description>
    </item>
    
    <item>
      <title>Using Linear Discriminant Analysis For Dimensionality Reduction</title>
      <link>/machine_learning/feature_engineering/lda_for_dimensionality_reduction/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_engineering/lda_for_dimensionality_reduction/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis Load Iris Data # Load the Iris flower dataset: iris = datasets.load_iris() X = iris.data y = iris.target Create A Linear # Create an LDA that will reduce the data down to 1 feature lda = LinearDiscriminantAnalysis(n_components=1) # run an LDA and use it to transform the features X_lda = lda.fit(X, y).transform(X) View Results # Print the number of features print(&amp;#39;Original number of features:&amp;#39;, X.</description>
    </item>
    
    <item>
      <title>Using Mean Color As A Feature</title>
      <link>/machine_learning/preprocessing_images/using_mean_color_as_a_feature/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/preprocessing_images/using_mean_color_as_a_feature/</guid>
      <description>Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as BGR image_bgr = cv2.imread(&amp;#39;images/plane_256x256.jpg&amp;#39;, cv2.IMREAD_COLOR) Calculate Mean Color Of Each Color Channel # Calculate the mean of each channel channels = cv2.mean(image_bgr) # Swap blue and red values (making it RGB, not BGR) observation = np.array([(channels[2], channels[1], channels[0])]) Show Values # Show mean channel values observation array([[ 90.</description>
    </item>
    
    <item>
      <title>Variance Thresholding Binary Features</title>
      <link>/machine_learning/feature_selection/variance_thresholding_binary_features/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/variance_thresholding_binary_features/</guid>
      <description>Preliminaries from sklearn.feature_selection import VarianceThreshold Load Data # Create feature matrix with:  # Feature 0: 80% class 0 # Feature 1: 80% class 1 # Feature 2: 60% class 0, 40% class 1 X = [[0, 1, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0]] Conduct Variance Thresholding In binary features (i.e. Bernoulli random variables), variance is calculated as:
$$\operatorname {Var} (x)= p(1-p)$$</description>
    </item>
    
    <item>
      <title>Variance Thresholding For Feature Selection</title>
      <link>/machine_learning/feature_selection/variance_thresholding_for_feature_selection/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/feature_selection/variance_thresholding_for_feature_selection/</guid>
      <description>Preliminaries from sklearn import datasets from sklearn.feature_selection import VarianceThreshold Load Data # Load iris data iris = datasets.load_iris() # Create features and target X = iris.data y = iris.target Conduct Variance Thresholding # Create VarianceThreshold object with a variance with a threshold of 0.5 thresholder = VarianceThreshold(threshold=.5) # Conduct variance thresholding X_high_variance = thresholder.fit_transform(X) View high variance features # View first five rows with features with variances above threshold X_high_variance[0:5] array([[ 5.</description>
    </item>
    
    <item>
      <title>Visualize A Decision Tree</title>
      <link>/machine_learning/trees_and_forests/visualize_a_decision_tree/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/trees_and_forests/visualize_a_decision_tree/</guid>
      <description>Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets from IPython.display import Image from sklearn import tree import pydotplus Load Iris Data # Load data iris = datasets.load_iris() X = iris.data y = iris.target Train Decision Tree # Create decision tree classifer object clf = DecisionTreeClassifier(random_state=0) # Train model model = clf.fit(X, y) Visualize Decision Tree # Create DOT data dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names) # Draw graph graph = pydotplus.</description>
    </item>
    
    <item>
      <title>k-Means Clustering</title>
      <link>/machine_learning/clustering/k-means_clustering/</link>
      <pubDate>Wed, 20 Dec 2017 11:53:49 -0700</pubDate>
      
      <guid>/machine_learning/clustering/k-means_clustering/</guid>
      <description>Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Load Iris Flower Dataset # Load data iris = datasets.load_iris() X = iris.data Standardize Features # Standarize features scaler = StandardScaler() X_std = scaler.fit_transform(X) Conduct k-Means Clustering # Create k-mean object clt = KMeans(n_clusters=3, random_state=0, n_jobs=-1) # Train model model = clt.fit(X_std) Show Each Observation&amp;rsquo;s Cluster Membership # View predict class model.labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)  Create New Observation # Create new observation new_observation = [[0.</description>
    </item>
    
  </channel>
</rss>