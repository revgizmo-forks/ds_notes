<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Chris Albon  | K-Nearest Neighbors Classification</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.56.3" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="K-Nearest Neighbors Classification" />
<meta property="og:description" content="A quick guide to using k-nearest neighbor using numpy and scikit." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/" />
<meta property="article:published_time" content="2017-12-20T11:53:49-07:00" />
<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="name" content="K-Nearest Neighbors Classification">
<meta itemprop="description" content="A quick guide to using k-nearest neighbor using numpy and scikit.">


<meta itemprop="datePublished" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="dateModified" content="2017-12-20T11:53:49-07:00" />
<meta itemprop="wordCount" content="626">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K-Nearest Neighbors Classification"/>
<meta name="twitter:description" content="A quick guide to using k-nearest neighbor using numpy and scikit."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="" class="f3 fw2 hover-white no-underline white-90 dib">
      Chris Albon
    </a>
    <div class="flex-l items-center">
      

      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        MACHINE_LEARNINGS
      </p>
      <h1 class="f1 athelas mb1">K-Nearest Neighbors Classification</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2017-12-20T11:53:49-07:00">December 20, 2017</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l">

<p>K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.</p>

<p>KNN has three basic parts:</p>

<ul>
<li>$y_i$: The class of an observation (what we are trying to predict in the test data).</li>
<li>$X_i$: The predictors/IVs/attributes of an observation.</li>
<li>$K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its &ldquo;neighborhood&rdquo;. For example, K=2 means that each observation&rsquo;s has a neighorhood comprising of the two other observations closest to it.</li>
</ul>

<p>Imagine we have an observation where we know its independent variables $x_{test}$ but do not know its class $y_{test}$. The KNN learner finds the K other observations that are closest to $x_{test}$ and uses their known classes to assign a classes to $_{test}$.</p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>  
<span class="kn">import</span> <span class="nn">seaborn</span></code></pre></div>
<h2 id="create-dataset">Create Dataset</h2>

<p>Here we create three variables, <code>test_1</code> and <code>test_2</code> are our independent variables, &lsquo;outcome&rsquo; is our dependent variable. We will use this data to train our learner.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">training_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;test_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3051</span><span class="p">,</span><span class="mf">0.4949</span><span class="p">,</span><span class="mf">0.6974</span><span class="p">,</span><span class="mf">0.3769</span><span class="p">,</span><span class="mf">0.2231</span><span class="p">,</span><span class="mf">0.341</span><span class="p">,</span><span class="mf">0.4436</span><span class="p">,</span><span class="mf">0.5897</span><span class="p">,</span><span class="mf">0.6308</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span>
<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;test_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5846</span><span class="p">,</span><span class="mf">0.2654</span><span class="p">,</span><span class="mf">0.2615</span><span class="p">,</span><span class="mf">0.4538</span><span class="p">,</span><span class="mf">0.4615</span><span class="p">,</span><span class="mf">0.8308</span><span class="p">,</span><span class="mf">0.4962</span><span class="p">,</span><span class="mf">0.3269</span><span class="p">,</span><span class="mf">0.5346</span><span class="p">,</span><span class="mf">0.6731</span><span class="p">]</span>
<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>

<span class="n">training_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test_1</th>
      <th>test_2</th>
      <th>outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.3051</td>
      <td>0.5846</td>
      <td>win</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.4949</td>
      <td>0.2654</td>
      <td>win</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.6974</td>
      <td>0.2615</td>
      <td>win</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3769</td>
      <td>0.4538</td>
      <td>win</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2231</td>
      <td>0.4615</td>
      <td>win</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="plot-the-data">Plot the data</h2>

<p>This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">seaborn</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s1">&#39;test_1&#39;</span><span class="p">,</span> <span class="s1">&#39;test_2&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="s2">&#34;outcome&#34;</span><span class="p">,</span> <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;marker&#34;</span><span class="p">:</span> <span class="s2">&#34;D&#34;</span><span class="p">,</span><span class="s2">&#34;s&#34;</span><span class="p">:</span> <span class="mi">100</span><span class="p">})</span></code></pre></div>
<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x11008aeb8&gt;
</code></pre>

<p><img src="k-nearest_neighbors_classifer_9_1.png" alt="png" /></p>

<h2 id="convert-data-into-np-arrays">Convert Data Into np.arrays</h2>

<p>The <code>scikit-learn</code> library requires the data be formatted as a <code>numpy</code> array. Here are doing that reformatting.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">training_data</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;test_1&#39;</span><span class="p">,</span> <span class="s1">&#39;test_2&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">])</span></code></pre></div>
<h2 id="train-the-learner">Train The Learner</h2>

<p>This is our big moment. We train a KNN learner using the parameters that an observation&rsquo;s neighborhood is its three closest neighors. <code>weights = 'uniform'</code> can be thought of as the voting system used. For example, <code>uniform</code> means that all neighbors get an equally weighted &ldquo;vote&rdquo; about an observation&rsquo;s class while <code>weights = 'distance'</code> would tell the learner to weigh each observation&rsquo;s &ldquo;vote&rdquo; by its distance from the observation we are classifying.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="n">trained_model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<h2 id="view-the-model-s-score">View The Model&rsquo;s Score</h2>

<p>How good is our trained model compared to our training data?</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">trained_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>0.80000000000000004
</code></pre>

<p>Our model is 80% accurate!</p>

<p><em>Note: that in any real world example we&rsquo;d want to compare the trained model to some holdout test data. But since this is a toy example I used the training data</em>.</p>

<h2 id="apply-the-learner-to-a-new-data-point">Apply The Learner To A New Data Point</h2>

<p>Now that we have trained our model, we can predict the class any new observation, $y_{test}$. Let us do that now!</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Create a new observation with the value of the first independent variable, &#39;test_1&#39;, as .4 </span>
<span class="c1"># and the second independent variable, test_1&#39;, as .6 </span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">4</span><span class="p">,</span><span class="o">.</span><span class="mi">6</span><span class="p">]])</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Apply the learner to the new, unclassified observation.</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span></code></pre></div>
<pre><code>array(['loss'], dtype=object)
</code></pre>

<p>Huzzah! We can see that the learner has predicted that the new observation&rsquo;s class is <code>loss</code>.</p>

<p>We can even look at the probabilities the learner assigned to each class:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">trained_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span></code></pre></div>
<pre><code>array([[ 0.66666667,  0.33333333]])
</code></pre>

<p>According to this result, the model predicted that the observation was <code>loss</code> with a ~67% probability and <code>win</code> with a ~33% probability. Because the observation had a greater probability of being <code>loss</code>, it predicted that class for the observation.</p>

<h2 id="notes">Notes</h2>

<ul>
<li>The choice of K has major affects on the classifer created.</li>
<li>The greater the K, more linear (high bias and low variance) the decision boundary.</li>
<li>There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity.</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="" >
    &copy; 2019 Chris Albon
  </a>
    <div>










</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
